## Support Vector Machine

### Model Description

In this section, we provide an overview of the Support Vector Machine (SVM) algorithm and how it is used.

Roughly speaking, the SVM model generates a hyper-plane or curve that splits the hyper-space into a Yes region (cancer recurs) and a No region (cancer doesn't recur). If a given a new patient represented by sample point falls in the Yes region, it is likely that the patient's cancer will recur. If the point falls in the No region, it is likely that the patient's cancer will not recur. We will refer to the Yes region as the class $1$ and the No region as the class $-1$.

### Maximal Margin Classifier

SVM is a generalization of the simple "maximal margin classifier." Suppose we are given training observations $x_1,\dots, x_n$ in a $p$-dimensional space, and each is in one of two classes: $1$ and $-1$. Based on the training observations, we would like to develop a classifier that correctly classifies an arbitrary test observation $x^*$. For each observation $x_i$, the value of its $j$th feature is denoted by $x_{ij}$.

The idea is to construct a hyperplane that exactly separates the training observations so that those labeled $1$ are on one side and those labeled $-1$ are on the other. In other words,

  - $\beta_0+\beta_1x_{i1}+\dots+\beta_px_{ip} > 0$ if the observation is labeled with $1$
  - $\beta_0+\beta_1x_{i1}+\dots+\beta_px_{ip} < 0$ if the observation is labeled with $-1$.

Thus, we can classify $x^*$ using the sign of $f(x^*) = \beta_0 + \beta_1x_1^* + \dots + \beta_px_p^*$, where we put it in class $1$ if $f(x^*)$ is positive and class $-1$ otherwise. The magnitude of $f(x^*)$ tells us more: greater $|f(x^*)|$ indicates that the point is farther from the hyperplane, so we are more confident in our classification.

Notice that when a separating hyperplane exists, there will actually be an infinite number of such hyperplanes. Define the *margin* of a hyperplane to be the minimum distance from a training observation to the hyperplane. Thus, we can define the *maximal margin classifier* to be the hyperplane with the maximum margin.

### Support Vector Classifier

The maximal margin classifier has two limitations: firt, a separating hyperplane does not always exist and second, it is very sensitive to outliers. For example, in @fig-limitation (modified from @james2021), the training observations are in the blue class or purple class, and the black point is a test observation. Originally, the maximal margin classifier puts the test observation in the purple class (see the left plot in @fig-limitation). After an outlier blue training observation is added, the maximal margin classifier line changes significantly and puts the test observation in the blue class (see the right plot in @fig-limitation), which may be undesirable because the test observation is much closer to the majority of purple points.

```{r}
#| label: fig-limitation
#| fig-cap: 'Limitations of the Maximal Margin Classifier.'

knitr::include_graphics(here::here('images/MaxMarginLimitations.png'))
```

Hence, we generalize the maximal margin classifier to the "support vector classifier," which works in the non-separable case and is not sensitive to outliers. In the support vector classifier, a hyperplane $\beta_0+\beta_1 x_{i1}+\dots + \beta_p x_{ip}=0$ that mostly separates the classes is constructed. A few observations could be either on the wrong side of the hyperplane or have a margin that is too small. See @fig-svclassifier for more details.

```{r}
#| label: fig-svclassifier
#| fig-cap: 'Support Vector Classifier.'

knitr::include_graphics(here::here('images/SupportVectorClassifier.png'))
```

This hyperplane can be found by solving the following optimization problem: $\text{maximize}_{\beta_0, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n, M}M$ subject to the conditions

  1.  $\sum_{j=1}^p \beta_j^2 = 1$
  2.  $y_i(\beta_0+\beta_1 x_{i1}+\dots + \beta_p x_{ip}) \ge M(1-\epsilon_i)$
  3.  $\epsilon_i \ge 0$ and $\sum_{i=1}^n \epsilon_i \le C$.

The goal of the optimization problem is to choose parameters that maximize $M$, the width of the margin.

Condition 1 requires that the sum of the squares of the coefficients equals 1. Notice that if this condition holds, then the signed distance from any point $x_i$ to the hyperplane simplifies to

$$
\frac{\beta_0+\beta_1 x_{i1} + \dots + \beta_p x_{ip}}{\sqrt{ \beta_0^2+\dots+\beta_p^2}} = \beta_0+\beta_1 x_{i1} + \dots + \beta_p x_{ijp}.
$$

Condition 2 requires that each training observation is "far enough" from the hyperplane. For each training observation $x_i$, $y_i$ equals $1$ if it is in one class and $-1$ if it is in the other class, so $y_i(\beta_0+\beta_1 x_{i1}+\dots + \beta_p x_{ip})$ is the unsigned distance of the observation to the hyperplane. Thus, condition 2 means that each training observation must have distance of at least $M(1-\epsilon_i)$ from the hyperplane.
    
Here $\epsilon_i$ is a small error term, where $\epsilon_i=0$ means that the $i$th observation is on the correct side of the margin, $\epsilon_i>0$ means that it is on the wrong side of the margin, and $\epsilon_i>1$ means that it is on the wrong side of the hyperplane. 

Condition 3 requires that all error terms $\epsilon_i$ are non-negative and their sum is at most $C$, where $C$ is tuning parameter that we can choose when applying the support vector classifier. Higher $C$ corresponds to higher tolerance of violations of the margin.

A key property of the support vector classifier is that the only observations that affect the hyperplane are the ones on the margin or violating it; these observations are called "support vectors." In other words, if we move around any other observation so that it is still on the correct side, the hyperplane remains unchanged. This is an advantage because unlike other classifying methods, the support vector classifier is not sensitive to extreme observations.

### Support Vector Machine

A drawback of the support vector classifier is that when the data has non-linear decision boundaries (e.g., one cluster of the blue class, one cluster of the purple class, and one cluster of the blue class), a single application of the support vector classifier cannot clearly delineate the two classes. Hence, the support vector machine is developed to automatically adapt the support vector classifier for non-linear decision boundaries.

The idea is to enlarge the feature space using "kernels." First, observe that the support vector classifier can be computed entirely in terms of inner products of the observations, where the inner product of two observations $x_a, x_b$ is denoted by $\langle x_a, x_b \rangle = \sum_{i=1}^p x_{ai}x_{bi}$. For the SVM, we replace the inner product by a generalization $K(x_a, x_b)$, called the kernel. Let us go over two common kernels: the polynomial kernel and the radial kernel.

The _polynomial kernel_ is defined as $K(x_a, x_b) = (1+\sum_{i=1}^p x_{ai}x_{bi})^d$ for some positive integer $d$. Essentially, an SVM with the polynomial kernel maps the observations into a higher-dimensional space (with polynomials of degree $d$) and then fits a support vector classifier to it.

Moreover, the _radial kernel_ is defined as $K(x_a, x_b) = \text{exp}(-\gamma \sum_{i=1}^p (x_{ai}-x_{bi})^2)$ for some positive constant $\gamma$. When classifying the test observation $x^*$, the radial kernel prioritizes nearby training observations. More precisely, the closer $x^*$ is from a training observation $x_a$, the smaller the distance $\sum_{i=1}^p (x_{ai} - x_i^*)^2$, the larger the radial kernel $K(x_a, x^*)$, and so the more important $x_a$ will be in determining $f(x^*)$.

The @fig-svm-kernels shows data with a non-linear boundary between the blue (class $1$) and purple (class $-1$) points. The left side shows the black boundary curves generated by an SVM with a polynomial kernel of degree $3$, and the right side shows the circular black boundary generated by an SVM with a radial kernel. In this case, both kernels work well to split the two classes.

```{r}
#| label: fig-svm-kernels
#| fig-cap: 'Using Kernels to Determine a Non-Linear Boundary Between Classes.'

knitr::include_graphics(here::here('images/Kernels.png'))
```

### Model Workflow

```{r, warning=FALSE, message=FALSE}

# Read cleaned data.
cleaned_data <- readr::read_csv(here::here('data', 'cleaned-data.csv'))

```


```{r svm-data-split, cache=TRUE}

set.seed(314)

# Split data into training, validation, and test sets.
data_split <- rsample::initial_validation_split(cleaned_data)
train_data <- rsample::training(data_split)
validation_data <- rsample::validation(data_split)
test_data <- rsample::testing(data_split)

# Create cross-validation collection.
data_cross_val <- rsample::vfold_cv(train_data)

# Save the test data `Recurred`.
test_outcome <- 
  factor(test_data$Recurred)

```


```{r svm-data-recipes, cache=TRUE}

# Create recipe for the data prep.
data_rec <- recipes::recipe(Recurred ~ ., data = train_data) |>
  recipes::step_corr(threshold = 0.6) |>
  recipes::step_normalize(recipes::all_numeric()) |>
  recipes::step_dummy(recipes::all_nominal_predictors())

```

We will create an SVM model specification and workflow indicating the model hyper-parameters: a cost value and `rbf_sigma`, a positive number for radial basis function. To optimize our model, we will use the `tune::tune()` function to find optimal values of these parameters in terms of model accuracy.

```{r svm-workflow, cache=TRUE, echo=TRUE}

# Create model specification.
svm_model_spec <-
  parsnip::svm_rbf(cost = tune::tune(), rbf_sigma = tune::tune()) |>
  parsnip::set_mode('classification') |>
  parsnip::set_engine('kernlab')

# Create model workflow.
svm_workflow <- workflows::workflow() |>
  workflows::add_model(svm_model_spec) |>
  workflows::add_recipe(data_rec)

```


### Model Tuning and Fitting

As we did for KNN, we use parallel computing to fine-tuning our model using the $10$-fold cross validation we set up earlier. We end this section by selecting the best model based on accuracy and the ROC curve.


```{r svm-param-tunning, cache=TRUE, warning=FALSE, echo=TRUE}

#' Check number of available cores.
cores_no <- parallel::detectCores() - 1

#' Start timer.
tictoc::tic()

# Create cores_no clusters.
clusters <- parallel::makeCluster(cores_no)

# Start clusters.
doParallel::registerDoParallel(clusters)

# Fine-tune the model params.
svm_res <- tune::tune_grid(
  object = svm_workflow,
  preprocessor = svm_model_spec,
  resamples = data_cross_val,
  control = tune::control_resamples(save_pred = TRUE)
)

# Select 'best' fit (in terms of accuracy).
svm_best_fit <- 
  svm_res |> 
  tune::select_best(metric = 'accuracy')

# Use the 'best' model params for our final model.
svm_final_workflow <- 
  svm_workflow |>
  tune::finalize_workflow(svm_best_fit)

# Fit our model using the 'best' params.
svm_final_fit <- 
  svm_final_workflow |> 
  tune::last_fit(data_split)

# Stop clusters.
parallel::stopCluster(clusters)

# Stop timer.
tictoc::toc()

# Choose the best model params.
svm_res |> tune::select_best(metric = 'roc_auc')

```

### Model Performance

We then apply our selected model to the test set. The final metrics are given in @tbl-svm-performance.

```{r svn-performance, cache=TRUE, echo=TRUE}

# Use the best fit to make predictions on the test data.
svm_pred <- 
  svm_final_fit |> 
  tune::collect_predictions() |>
  dplyr::mutate(truth = factor(.pred_class))

```


```{r}
#| label: tbl-svm-performance
#| tbl-cap: 'SVM Performance Metrics: Accuracy, Precision, Recall, and Specificity.'
#| tbl-alt: 'SVM Performance Metrics: Accuracy, Precision, Recall, and Specificity.'

# Prepare table's theme.
theme <- reactable::reactableTheme(
  borderColor = "#dfe2e5",
  stripedColor = "#f6f8fa", 
  highlightColor = "#f0f5f9",
  cellPadding = "8px 12px"
)

# Create metrics table.
svm_metrics_table <- list(
  'Accuracy' = yardstick::accuracy_vec(truth = svm_pred[['.pred_class']],
                                       estimate = test_outcome),
  'Precision' = yardstick::precision_vec(truth = svm_pred[['.pred_class']],
                                         estimate = test_outcome),
  'Recall' = yardstick::recall_vec(truth = svm_pred[['.pred_class']],
                                   estimate = test_outcome),
  'Specificity' = yardstick::specificity_vec(truth = svm_pred[['.pred_class']],
                                            estimate = test_outcome)
) |>
  dplyr::bind_cols() |>
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>
  dplyr::mutate(Value = round(Value*100, 1))

readr::write_csv(x = svm_metrics_table, file = here::here('data', 'svm-metrics.csv'))

svm_metrics_table |>
  dplyr::mutate(Value = paste0(Value, '%')) |>
  reactable::reactable(
    searchable = FALSE, 
    resizable = TRUE,
    onClick = "expand",
    bordered = TRUE,
    highlight = TRUE, 
    compact = TRUE,
    height = "auto",
    theme = theme
  )

```