## Random Forest

### Model Description

Random forest is an ensemble method based on regression or classification trees. We will focus on classification trees, because the target variable in the Thyroid data set is categorical. These trees divide the predictor space into multiple simple regions, and given an observation, we predict that it falls into the most commonly occurring class of training observations in the region where it belongs. For example, consider a classification tree that splits into two branches based on whether $\text{Age}<35$ and splits into two sub-branches based on whether $\text{Gender=Female}$, in order to predict cancer recurrence. If a new observation has $\text{Age}=50$ and $\text{Gender=Female}$, we would follow the $\text{Age}\ge35$ and $\text{Gender=Female}$ branch to find the prediction. To construct a classification tree, we use recursive binary splits, and each time we choose the split that minimizes the error rate (common measures of error rate are the classification error rate, Gini index, and entropy [@james2021]).

However, classification trees have a limitation: they have high varianceâ€”in other words, depending on how the training data is split, the resulting tree can be very different. To improve performance, we apply bagging: choose some large integer $B$ and using $B$ bootstrapped training sets, construct $B$ classification trees and take the majority of the resulting predictions. In this way, variance is reduced, because if the variance of each of $B$ independent observations $Z_1, \dots, Z_B$ is $\sigma^2$, then the variance of the mean of the observations is $\sigma^2/B$.

Finally, the random forest method improves on bagging in the following way: when constructing decision trees, we randomly choose $m$ predictors to be split candidates, and the split can only use one of these $m$ predictors. We usually use $m \approx \sqrt{p}$, where $p$ is the number of predictors. It may seem counterintuitive that using fewer predictors in each split is an improvement, but the reasoning is that for bagged trees, since the strongest predictors are used in the top split, the bagged trees are very similar to each other, so their predictions are highly correlated. However, averaging correlated quantities leads to a smaller decrease in variance than averaging uncorrelated quantities. Thus, since the randomization in random forests helps decorrelate the trees, the overall predictions will have less variance.

### Model Workflow

```{r}
# Read cleaned data.
cleaned_data <- readr::read_csv(here::here('data', 'cleaned-data.csv'))
```

```{r rf-data-split, cache=TRUE}

# Initialize a random number generator.
set.seed(314)

# Split data into training and test sets.
data_split <- rsample::initial_split(cleaned_data)
train_data <- rsample::training(data_split)
test_data <- rsample::testing(data_split)

# Split the training data into 10 folds for cross-validation.
data_cross_val <- rsample::vfold_cv(train_data)

# Set aside the outcome column of the sample test data.
test_outcome <- 
  factor(test_data$Recurred)

```

```{r rf-data-recipes, cache=TRUE}
library(tidymodels)

# Create recipe for the data prep.
data_rec <- recipes::recipe(Recurred ~ ., data = train_data) |>
  recipes::step_corr(threshold = 0.6) |>
  recipes::step_normalize(recipes::all_numeric()) |>
  recipes::step_dummy(recipes::all_nominal_predictors())

```

```{r rf-workflow, cache=TRUE}

# Create model specification.
rf_model_spec <- 
  rand_forest(
    mtry = tune(),
    trees = 1000,
    min_n = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("ranger")

# Create model workflow.
rf_workflow <- workflows::workflow() |>
  workflows::add_model(rf_model_spec) |>
  workflows::add_recipe(data_rec)

```

### Model Tuning and Fitting

```{r rf-param-tunning, cache=TRUE, warning=FALSE, echo=TRUE}

#' Check number of available cores.
cores_no <- parallel::detectCores() - 1

#' Start timer.
tictoc::tic()

# Create cores_no clusters.
clusters <- parallel::makeCluster(cores_no)

# Start clusters.
doParallel::registerDoParallel(clusters)

# Fine-tune the model params.
rf_res <- tune::tune_grid(
  object = rf_workflow,
  preprocessor = rf_model_spec,
  resamples = data_cross_val,
  control = tune::control_resamples(save_pred = TRUE)
)

# Select 'best' fit (in terms of accuracy).
rf_best_fit <- 
  rf_res |> 
  tune::select_best(metric = 'accuracy')

# Use the 'best' model params for our final model.
rf_final_workflow <- 
  rf_workflow |>
  tune::finalize_workflow(rf_best_fit)

# Fit our model using the 'best' params.
rf_final_fit <- 
  rf_final_workflow |> 
  tune::last_fit(data_split)

# Stop clusters.
parallel::stopCluster(clusters)

# Stop timer.
tictoc::toc()

# Choose the best model params.
rf_res |> tune::select_best(metric = 'roc_auc')

```

### Model Performance

We then apply our selected model to the test set. The final metrics are given in @tbl-rf-performance.

```{r, cache=TRUE, echo=TRUE}

# Use the best fit to make predictions on the test data.
rf_pred <- 
  rf_final_fit |> 
  tune::collect_predictions() |>
  dplyr::mutate(truth = factor(.pred_class))

```

```{r}
#| label: tbl-rf-performance
#| tbl-cap: 'Random Forest Performance Metrics: Accuracy, Precision, Recall, and Specificity.'
#| tbl-alt: 'Random Forest Performance Metrics: Accuracy, Precision, Recall, and Specificity.'

# Prepare table's theme.
theme <- reactable::reactableTheme(
  borderColor = "#dfe2e5",
  stripedColor = "#f6f8fa", 
  highlightColor = "#f0f5f9",
  cellPadding = "8px 12px"
)

# Create metrics table.
rf_metrics_table <- list(
  'Accuracy' = yardstick::accuracy_vec(truth = rf_pred[['.pred_class']],
                                       estimate = test_outcome),
  'Precision' = yardstick::precision_vec(truth = rf_pred[['.pred_class']],
                                         estimate = test_outcome),
  'Recall' = yardstick::recall_vec(truth = rf_pred[['.pred_class']],
                                   estimate = test_outcome),
  'Specificity' = yardstick::specificity_vec(truth = rf_pred[['.pred_class']],
                                            estimate = test_outcome)
) |>
  dplyr::bind_cols() |>
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>
  dplyr::mutate(Value = round(Value*100, 1))

readr::write_csv(x = rf_metrics_table, file = here::here('data', 'rf-metrics.csv'))

rf_metrics_table |>
  dplyr::mutate(Value = paste0(Value, '%')) |>
  reactable::reactable(
    searchable = FALSE, 
    resizable = TRUE,
    onClick = "expand",
    bordered = TRUE,
    highlight = TRUE, 
    compact = TRUE,
    height = "auto",
    theme = theme
  )

```
