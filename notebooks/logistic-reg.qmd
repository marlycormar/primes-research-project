## Logistic Regression

### Model Description
Logistic regression for binary classification is a supervised algorithm which predicts the probability a data point $X$ belongs to each category. This model is essentially a modification of the linear regression model to instead output probabilities by applying the softmax function. Let "$1$" represent the "positive" class and "$0$" represent the "negative" class. The probability of $X$ belonging to the category "$1$" is given as
$$p(X) = p(Y = 1 \mid X) = \text{Softmax}(\beta_0 + \beta_1X_1 + \dots + \beta_pX_p) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1X_1 + \dots + \beta_p X_p}},$$
where $\beta_0, \beta_1, \dots, \beta_p$ are parameters to be estimated. 
We classify $X$ as "positive" if $p(X) > 0.5$.

By construction, we have
$$\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X_1 + \dots \beta_pX_p},$$
and this quantity is called the "odds" -- as it denotes the estimated probability of recurrence over no recurrence. It follows that the quantity
$$\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X_1 + \dots \beta_pX_p$$
called the "log odds" is linear.


We find the optimal parameters using the maximum likelihood function
$$L(\beta_0, \dots, \beta_p) = \prod_{i: y_i = 1}p(x_i) \prod_{j: y_j = 0}(1 - p(x_j)).$$
(Note that this maximizes the probability of the observed training data.) Or equivalently, we want to maximize the *log likelihood function*
$$LLF = \sum_{i = 1}^n \left[y_i\log(p(x_i)) + (1 - y_i)\log(1 - p(x_i))\right].$$




### Model Workflow

```{r lr-load-data}
# Read cleaned data.
cleaned_data <- readr::read_csv(here::here('data', 'cleaned-data.csv'))

# Load the data already computed in the index.qmd file.
data_split <- readRDS(here::here('data/data_split.rds'))
data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))
test_outcome <- readRDS(here::here('data/test_outcome.rds'))
data_rec <- readRDS(here::here('data/data_rec.rds'))

```

```{r lr-workflow, cache=TRUE, echo=TRUE}

# Create model specification.
# Setting mixture = 1 means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.
lr_model_spec <-
  parsnip::logistic_reg(penalty = tune(), mixture = 1) |>
  parsnip::set_mode('classification') |>
  parsnip::set_engine('glmnet')

# Create model workflow.
lr_workflow <- workflows::workflow() |>
  workflows::add_model(lr_model_spec) |>
  workflows::add_recipe(data_rec)

```

### Model Tuning and Fitting


```{r lr-param-tunning, cache=TRUE, warning=FALSE, echo=TRUE}

#' Check number of available cores.
cores_no <- parallel::detectCores() - 1

#' Start timer.
tictoc::tic()

# Create cores_no clusters.
clusters <- parallel::makeCluster(cores_no)

# Start clusters.
doParallel::registerDoParallel(clusters)

# Fine-tune the model params.
lr_res <- tune::tune_grid(
  object = lr_workflow,
  preprocessor = lr_model_spec,
  resamples = data_cross_val,
  control = tune::control_resamples(save_pred = TRUE)
)

# Select 'best' fit (in terms of accuracy).
lr_best_fit <- 
  lr_res |> 
  tune::select_best(metric = 'accuracy')

# Use the 'best' model params for our final model.
lr_final_workflow <- 
  lr_workflow |>
  tune::finalize_workflow(lr_best_fit)

# Fit our model using the 'best' params.
lr_final_fit <- 
  lr_final_workflow |> 
  tune::last_fit(data_split)

# Stop clusters.
parallel::stopCluster(clusters)

# Stop timer.
tictoc::toc()

# Choose the best model params.
lr_res |> tune::select_best(metric = 'roc_auc')

```

### Model Performance

We then apply our selected model to the test set. The final metrics are given in @tbl-lr-performance.

```{r, cache=TRUE, echo=TRUE}

# Use the best fit to make predictions on the test data.
lr_pred <- 
  lr_final_fit |> 
  tune::collect_predictions() |>
  dplyr::mutate(truth = factor(.pred_class))

```


```{r}
#| label: tbl-lr-performance
#| tbl-cap: 'Logistic Regression Performance Metrics: Accuracy, Precision, Recall, and Specificity.'
#| tbl-alt: 'Logistic Regression Performance Metrics: Accuracy, Precision, Recall, and Specificity.'

# Prepare table's theme.
theme <- reactable::reactableTheme(
  borderColor = "#dfe2e5",
  stripedColor = "#f6f8fa", 
  highlightColor = "#f0f5f9",
  cellPadding = "8px 12px"
)

# Create metrics table.
lr_metrics_table <- list(
  'Accuracy' = yardstick::accuracy_vec(truth = lr_pred[['.pred_class']],
                                       estimate = test_outcome),
  'Precision' = yardstick::precision_vec(truth = lr_pred[['.pred_class']],
                                         estimate = test_outcome),
  'Recall' = yardstick::recall_vec(truth = lr_pred[['.pred_class']],
                                   estimate = test_outcome),
  'Specificity' = yardstick::specificity_vec(truth = lr_pred[['.pred_class']],
                                            estimate = test_outcome)
) |>
  dplyr::bind_cols() |>
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>
  dplyr::mutate(Value = round(Value*100, 1))

readr::write_csv(x = lr_metrics_table, file = here::here('data', 'lr-metrics.csv'))

lr_metrics_table |>
  dplyr::mutate(Value = paste0(Value, '%')) |>
  reactable::reactable(
    searchable = FALSE, 
    resizable = TRUE,
    onClick = "expand",
    bordered = TRUE,
    highlight = TRUE, 
    compact = TRUE,
    height = "auto",
    theme = theme
  )

```