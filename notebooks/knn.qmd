## K-Nearest Neighbors

### Model Description

The K-Nearest Neighbors (KNN) algorithm is a nonparametric algorithm which classifies a given sample using the training data as a proximity reference. Given the training data, the algorithm classifies a point $X$ as the most common class label among its $k$ nearest training data points, where $k$ is a predetermined hyperparameter. The KNN classifier simply uses neighboring points to draw conclusions about a given point -- and so no model training takes place; the algorithm just memorizes the training data itself (this is usually referred to as a "lazy" algorithm).

The parameters of the classifier are the hyper-parameters $k$, the distance measure, and the weight function. Common examples distance measure used are the Euclidean distance, Manhattan distance, and Minkowski distance. The performance of the KNN classifier is obviously dependent on $k$ -- and generally, the optimal value of $k$ depends on the *bias-variance tradeoff*. Small values of $k$ result in low bias, but high variance, which means the training error is low, but test error is high. On the other hand, large values of $k$ result in high bias, but low variance. This means that the training error itself is high, but the training and test performances are comparable, and so the model underfits. Also, it is recommended to choose $k$ such that no classification ties are possible. When we have two classes (like we do here), this means choosing an odd $k$. To increase model flexibility, we may also use a weighted version of the KNN algorithm (where each of the $k$ nearest neighbors is weighted inversely based on the distance from the test point). We tune each of these three parameters below.



### Model Workflow

```{r knn-load-data}
# Read cleaned data.
cleaned_data <- readr::read_csv(here::here('data', 'cleaned-data.csv'))

# Load the data already computed in the index.qmd file.
data_split <- readRDS(here::here('data/data_split.rds'))
data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))
test_outcome <- readRDS(here::here('data/test_outcome.rds'))
data_rec <- readRDS(here::here('data/data_rec.rds'))

```

Now let us create a KNN model specification and workflow indicating the model hyper-parameters: a number of neighbors, a weight function, and a distance function. To optimize our model, we will use the `tune::tune()` function to find optimal values of these parameters in terms of model accuracy. 

```{r knn-workflow, cache=TRUE, echo=TRUE}

# Create model specification.
knn_model_spec <-
  parsnip::nearest_neighbor(
    neighbors = tune::tune(),
    weight_func = tune::tune(),
    dist_power = tune::tune()
  ) |>
  parsnip::set_mode('classification') |>
  parsnip::set_engine('kknn')

# Create model workflow.
knn_workflow <- workflows::workflow() |>
  workflows::add_model(knn_model_spec) |>
  workflows::add_recipe(data_rec)

```

### Model Tuning and Fitting

We then move to actually running our prepared workflow. In order to make the computation as fast as possible, we use parallel computing, i.e., the computation is distributed and simultaneously done over different cores.

Now we fine-tune the model hyper-parameters (namely $k$, the distance function, and the weight function) using the $10$-fold cross validation we set up earlier. Then we select the best model based on accuracy and the ROC curve.

```{r knn-param-tunning, cache=TRUE, warning=FALSE, echo=TRUE}

#' Check number of available cores.
cores_no <- parallel::detectCores() - 1

#' Start timer.
tictoc::tic()

# Create cores_no clusters.
clusters <- parallel::makeCluster(cores_no)

# Start clusters.
doParallel::registerDoParallel(clusters)

# Fine-tune the model params.
knn_res <- tune::tune_grid(
  object = knn_workflow,
  preprocessor = knn_model_spec,
  resamples = data_cross_val,
  control = tune::control_resamples(save_pred = TRUE)
)

# Select 'best' fit (in terms of accuracy).
knn_best_fit <- 
  knn_res |> 
  tune::select_best(metric = 'accuracy')

# Use the 'best' model params for our final model.
knn_final_workflow <- 
  knn_workflow |>
  tune::finalize_workflow(knn_best_fit)

# Fit our model using the 'best' params.
knn_final_fit <- 
  knn_final_workflow |> 
  tune::last_fit(data_split)

# Stop clusters.
parallel::stopCluster(clusters)

# Stop timer.
tictoc::toc()

# Choose the best model params.
knn_res |> tune::select_best(metric = 'roc_auc')

```


```{r}
# knn_res |> tune::collect_metrics()
# knn_final_fit |> tune::collect_metrics()

```

### Model Performance

We then apply our selected model to the test set. The final metrics are given in @tbl-knn-performance.

```{r, cache=TRUE, echo=TRUE}

# Use the best fit to make predictions on the test data.
knn_pred <- 
  knn_final_fit |> 
  tune::collect_predictions() |>
  dplyr::mutate(truth = factor(.pred_class))

```


```{r}
#| label: tbl-knn-performance
#| tbl-cap: 'KNN Performance Metrics: Accuracy, Precision, Recall, and Specificity.'
#| tbl-alt: 'KNN Performance Metrics: Accuracy, Precision, Recall, and Specificity.'

# Prepare table's theme.
theme <- reactable::reactableTheme(
  borderColor = "#dfe2e5",
  stripedColor = "#f6f8fa", 
  highlightColor = "#f0f5f9",
  cellPadding = "8px 12px"
)

# Create metrics table.
knn_metrics_table <- list(
  'Accuracy' = yardstick::accuracy_vec(truth = knn_pred[['.pred_class']],
                                       estimate = test_outcome),
  'Precision' = yardstick::precision_vec(truth = knn_pred[['.pred_class']],
                                         estimate = test_outcome),
  'Recall' = yardstick::recall_vec(truth = knn_pred[['.pred_class']],
                                   estimate = test_outcome),
  'Specificity' = yardstick::specificity_vec(truth = knn_pred[['.pred_class']],
                                            estimate = test_outcome)
) |>
  dplyr::bind_cols() |>
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>
  dplyr::mutate(Value = round(Value*100, 1))

readr::write_csv(x = knn_metrics_table, file = here::here('data', 'knn-metrics.csv'))

knn_metrics_table |>
  dplyr::mutate(Value = paste0(Value, '%')) |>
  reactable::reactable(
    searchable = FALSE, 
    resizable = TRUE,
    onClick = "expand",
    bordered = TRUE,
    highlight = TRUE, 
    compact = TRUE,
    height = "auto",
    theme = theme
  )

```