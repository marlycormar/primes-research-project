## Gradient Boosting Machines (GBMs) - XGBoost

### Model Description
Gradient boosting is an ensemble learning method, meaning that it combines the results of multiple different models (generally decision trees) into one. It does this sequentially, involving function minimization with gradient descent (hence the name). XGBoost, or eXtreme Gradient Boosting, is a popular algorithm for gradient boosting. It is designed to be efficient, flexible, and portable. It employs regularization techniques to prevent overfitting, and allows users to define their own object functions, making it adaptable to various types of supervised learning tasks, including regression, classification, and ranking problems. XGBoost also uses a technique called tree pruning to control the complexity of individual trees within the ensemble, preventing them from growing too deep and reducing the likelihood of overfitting.

Let's dive into the details. Consider the following flowchart of the model, from @xgboostarticle.

```{r}
#| label: fig-xgboost-overview
#| fig-cap: 'Flowchart of the XGBoost algorithm.'

knitr::include_graphics(here::here('images/XGBoost.png'))
```

We start with a simple model that returns a constant value (a tree with a single leaf node), $f_0(X)$. It then learns a new model $f_1(X, \theta_1)$ which fits the residual of $f_0(X)$. Then, for $2\le i\le k$, where $k$ is the pre-decided number of iterations, we compute the new model $f_i(X, \theta_i)$ that fits the residual of $f_{i-1}(X, \theta_i)$. The resulting model is $\hat{Y}=\sum_{i=1}^k f_i(X,\theta_i)$, the sum of the predictions from each model. The key step is how each $f_i$ model is recursively computed to fit the residual of the previous model. We need a loss function $L(Y, f(X))$ and a learning rate $\alpha$. At each step, we compute the gradient and hessian $\hat{g}_i$ and $\hat{h}_i$ by taking the first and second partial derivatives (respectively) of $L(Y, f_{i-1}(X))$ with respect to $f_{i-1}(x)$. We then compute the function $\theta_i$ with a minimization problem:
$$\theta_i(X)=\arg\min\sum_{x\in X}\hat{h}_m(x)\left(\theta_i(x)-\frac{\hat{g}_i(x)}{\hat{h}_i(x)}\right)^2.$$
The new function $f_i$ is now computed as $f_{i-1}(X) + \alpha\theta_i(X)$. 

### Model Workflow

```{r gbm-load-data}

library(tidymodels)

# Read cleaned data.
cleaned_data <- readr::read_csv(here::here('data', 'cleaned-data.csv'))

# Load the data already computed in the index.qmd file.
data_split <- readRDS(here::here('data/data_split.rds'))
data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))
test_outcome <- readRDS(here::here('data/test_outcome.rds'))
data_rec <- readRDS(here::here('data/data_rec.rds'))

# Set random seed.
set.seed(3145)

```

```{r gbm-workflow, cache=TRUE, echo=TRUE}

# Create model specification.
gbm_model_spec <- 
  boost_tree(
    trees = 1000,
    tree_depth = tune(), min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(), mtry = tune(),
    learn_rate = tune()
  ) |>
  set_engine('xgboost') |>
  set_mode('classification')

# Create model workflow.
gbm_workflow <- workflows::workflow() |>
  workflows::add_model(gbm_model_spec) |>
  workflows::add_recipe(data_rec)

```

### Model Tuning and Fitting


```{r gbm-param-tunning, cache=TRUE, warning=FALSE, echo=TRUE}

#' Check number of available cores.
cores_no <- parallel::detectCores() - 1

#' Start timer.
tictoc::tic()

# Create and register clusters.
clusters <- parallel::makeCluster(cores_no)
doParallel::registerDoParallel(clusters)

# Fine-tune the model params.
gbm_res <- tune::tune_grid(
  object = gbm_workflow,
  resamples = data_cross_val,
  control = tune::control_resamples(save_pred = TRUE)
)

# Select the best fit based on accuracy.
gbm_best_fit <- 
  gbm_res |> 
  tune::select_best(metric = 'accuracy')

# Finalize the workflow with the best parameters.
gbm_final_workflow <- 
  gbm_workflow |>
  tune::finalize_workflow(gbm_best_fit)

# Fit the final model using the best parameters.
gbm_final_fit <- 
  gbm_final_workflow |> 
  tune::last_fit(data_split)

# Stop clusters.
parallel::stopCluster(clusters)

# Stop timer.
tictoc::toc()

# Choose the best model parameters based on ROC AUC.
gbm_res |> tune::select_best(metric = 'roc_auc')

```

### Model Performance

We then apply our selected model to the test set. The final metrics are given in @tbl-gbm-performance.

```{r, cache=TRUE, echo=TRUE}

# Use the best fit to make predictions on the test data.
gbm_pred <- 
  gbm_final_fit |> 
  tune::collect_predictions() |>
  dplyr::mutate(truth = factor(.pred_class))

```


```{r}
#| label: tbl-gbm-performance
#| tbl-cap: 'XGBoost Performance Metrics: Accuracy, Precision, Recall, and Specificity.'
#| tbl-alt: 'XGBoost Performance Metrics: Accuracy, Precision, Recall, and Specificity.'

# Prepare table's theme.
theme <- reactable::reactableTheme(
  borderColor = "#dfe2e5",
  stripedColor = "#f6f8fa", 
  highlightColor = "#f0f5f9",
  cellPadding = "8px 12px"
)

# Create metrics table.
gbm_metrics_table <- list(
  'Accuracy' = yardstick::accuracy_vec(truth = gbm_pred[['.pred_class']],
                                       estimate = test_outcome),
  'Precision' = yardstick::precision_vec(truth = gbm_pred[['.pred_class']],
                                         estimate = test_outcome),
  'Recall' = yardstick::recall_vec(truth = gbm_pred[['.pred_class']],
                                   estimate = test_outcome),
  'Specificity' = yardstick::specificity_vec(truth = gbm_pred[['.pred_class']],
                                            estimate = test_outcome)
) |>
  dplyr::bind_cols() |>
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>
  dplyr::mutate(Value = round(Value*100, 1))

readr::write_csv(x = gbm_metrics_table, file = here::here('data', 'gbm-metrics.csv'))

gbm_metrics_table |>
  dplyr::mutate(Value = paste0(Value, '%')) |>
  reactable::reactable(
    searchable = FALSE, 
    resizable = TRUE,
    onClick = "expand",
    bordered = TRUE,
    highlight = TRUE, 
    compact = TRUE,
    height = "auto",
    theme = theme
  )

```