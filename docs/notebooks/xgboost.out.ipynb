{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "## Extreme Gradient Boosting\n",
    "\n",
    "### Model Description\n",
    "\n",
    "Extreme Gradient Boosting (XGBoost) is an advanced implementation of gradient boosting designed to enhance performance and speed. It builds upon the principles of gradient boosting to provide a highly efficient, flexible, and portable library that supports both regression and classification tasks. XGBoost has become one of the most popular machine learning algorithms due to its high performance and scalability.\n",
    "\n",
    "XGBoost operates by sequentially adding decision trees to an ensemble. Each tree is built to correct the errors of the previous trees in the ensemble. The process begins with an initial model, typically a simple model such as the mean of the target variable. At each subsequent step, a new decision tree is added to the model to predict the residuals (errors) of the previous trees. Each tree is built by optimizing an objective function that combines a loss function and a regularization term. The regularization term helps prevent overfitting by penalizing the complexity of the model. After each tree is added, the residuals are updated. The new tree aims to minimize these residuals, improving the overall model’s performance.\n",
    "\n",
    "The node splitting in each tree is guided by an objective function, which typically involves minimizing a loss function (such as mean squared error for regression or log loss for classification) while including a regularization term. The final prediction is the sum of the predictions from all the trees in the ensemble, effectively reducing variance. This process is depicted in the attached flowchart, showing how each tree contributes to the final model.\n",
    "\n",
    "XGBoost has several key advantages. It incorporates both L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting and manage model complexity. The algorithm supports parallel processing, significantly speeding up the training process. XGBoost can handle missing values internally, making it robust to incomplete datasets. Additionally, users can define custom objective functions and evaluation metrics, allowing for flexibility in optimization.\n",
    "\n",
    "### Model Workflow"
   ],
   "id": "e9f3a013-a4c3-4c82-baeb-6a7eb851425a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "✔ broom        1.0.6      ✔ recipes      1.0.10\n",
      "✔ dials        1.2.1      ✔ rsample      1.2.1 \n",
      "✔ dplyr        1.1.4      ✔ tibble       3.2.1 \n",
      "✔ ggplot2      3.5.1      ✔ tidyr        1.3.1 \n",
      "✔ infer        1.0.7      ✔ tune         1.2.1 \n",
      "✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n",
      "✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n",
      "✔ purrr        1.0.2      ✔ yardstick    1.3.1 "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "✖ purrr::discard() masks scales::discard()\n",
      "✖ dplyr::filter()  masks stats::filter()\n",
      "✖ dplyr::lag()     masks stats::lag()\n",
      "✖ recipes::step()  masks stats::step()\n",
      "• Use tidymodels_prefer() to resolve common conflicts."
     ]
    }
   ],
   "source": [
    "library(tidymodels)\n"
   ],
   "id": "cb27901d-7f1e-4e97-a134-0463dcfdab6f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To effectively train our XGBoost model and find the optimal hyperparameters, we will set up a workflow that includes model specification and data preprocessing. The hyperparameters to be tuned include:\n",
    "\n",
    "-   `tree_depth`: Controls the maximum depth of each tree, impacting the model’s complexity.\n",
    "-   `min_n`: Specifies the minimum number of observations that must exist in a node for a split to be attempted, preventing overly specific branches and encouraging generalization.\n",
    "-   `loss_reduction`: Sets the minimum reduction in the loss function required to make a further partition on a leaf node, helping to control overfitting by making the algorithm more conservative.\n",
    "-   `sample_size`: Determines the fraction of the training data used for fitting each individual tree, introducing randomness and preventing overfitting.\n",
    "-   `mtry`: Sets the number of features considered when looking for the best split, adding variability to enhance generalization.\n",
    "-   `learn_rate`: Also known as the shrinkage parameter, controls the rate at which the model learns. Smaller learning rates can lead to better performance by allowing the model to learn more slowly and avoid overfitting."
   ],
   "id": "5894a38c-46d6-425b-9940-ab4633005ae2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification.\n",
    "xgboost_model_spec <- \n",
    "  boost_tree(\n",
    "    trees = 1000,\n",
    "    tree_depth = tune(), \n",
    "    min_n = tune(),\n",
    "    loss_reduction = tune(),\n",
    "    sample_size = tune(), \n",
    "    mtry = tune(),\n",
    "    learn_rate = tune()\n",
    "  ) |>\n",
    "  set_engine('xgboost') |>\n",
    "  set_mode('classification')\n",
    "\n",
    "# Create model workflow.\n",
    "xgboost_workflow <- workflows::workflow() |>\n",
    "  workflows::add_model(xgboost_model_spec) |>\n",
    "  workflows::add_recipe(data_rec)\n"
   ],
   "id": "a1e9edf8-91c2-4fa5-8b49-5466c3a58f83"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning and Fitting"
   ],
   "id": "0c66c3ba-427b-4938-ac1f-8992abe0866b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "i Creating pre-processing data to finalize unknown parameter: mtry"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12.105 sec elapsed"
     ]
    }
   ],
   "source": [
    "#' Check number of available cores.\n",
    "cores_no <- parallel::detectCores() - 1\n",
    "\n",
    "#' Start timer.\n",
    "tictoc::tic()\n",
    "\n",
    "# Create and register clusters.\n",
    "clusters <- parallel::makeCluster(cores_no)\n",
    "doParallel::registerDoParallel(clusters)\n",
    "\n",
    "# Fine-tune the model params.\n",
    "xgboost_res <- tune::tune_grid(\n",
    "  object = xgboost_workflow,\n",
    "  resamples = data_cross_val,\n",
    "  control = tune::control_resamples(save_pred = TRUE)\n",
    ")\n"
   ],
   "id": "07e3204b-3b77-449f-9f1c-f4ac387f24ef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "We then apply our selected model to the test set. The final metrics are given in @tbl-xgboost-performance-html."
   ],
   "id": "64ea8486-cc36-4aef-b042-57da9fa24971"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best fit to make predictions on the test data.\n",
    "xgboost_pred <- \n",
    "  xgboost_final_fit |> \n",
    "  tune::collect_predictions() |>\n",
    "  dplyr::mutate(truth = factor(.pred_class))\n"
   ],
   "id": "665cb67e-7b30-4cfc-b309-4778295bee7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table.\n",
    "xgboost_metrics_table <- list(\n",
    "  'Accuracy' = yardstick::accuracy_vec(truth = xgboost_pred[['.pred_class']],\n",
    "                                       estimate = test_outcome),\n",
    "  'Precision' = yardstick::precision_vec(truth = xgboost_pred[['.pred_class']],\n",
    "                                         estimate = test_outcome),\n",
    "  'Recall' = yardstick::recall_vec(truth = xgboost_pred[['.pred_class']],\n",
    "                                   estimate = test_outcome),\n",
    "  'Specificity' = yardstick::specificity_vec(truth = xgboost_pred[['.pred_class']],\n",
    "                                            estimate = test_outcome)\n",
    ") |>\n",
    "  dplyr::bind_cols() |>\n",
    "  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>\n",
    "  dplyr::mutate(Value = round(Value*100, 1))\n",
    "\n",
    "readr::write_csv(x = xgboost_metrics_table, file = here::here('data', 'xgboost-metrics.csv'))\n"
   ],
   "id": "4a0878b1-684c-4806-b831-03097e1a3186"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
