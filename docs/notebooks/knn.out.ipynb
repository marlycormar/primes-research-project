{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "## K-Nearest Neighbors\n",
    "\n",
    "### Model Description\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a nonparametric method used for classification. It classifies a given sample based on the proximity to the training data. The algorithm determines the class of a point $X$ by identifying the most common class label among its $k$ nearest neighbors, where $k$ is a predetermined hyperparameter. Unlike other algorithms, the KNN classifier does not involve training a model; instead, it memorizes the training data, making it a “lazy” algorithm.\n",
    "\n",
    "The primary hyperparameters of the KNN algorithm are $k$, the distance measure, and the weight function. Common distance measures include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "\n",
    "Choosing the optimal value for $k$ is crucial and involves balancing the bias-variance tradeoff. A small $k$ results in low bias and high variance. Low bias means the model captures the complexity of the training data very well, but high variance means the model is highly sensitive to the specifics of the training data, often leading to overfitting and higher test errors. As $k$ increases, the model averages over more neighbors, which smooths out the predictions and reduces the model’s sensitivity to individual data points, thus reducing variance. Therefore, a large $k$ results in high bias and low variance. The model may become too simplistic, leading to higher bias, but it becomes less sensitive to the training data, making it more robust to noise and better at generalizing to new data.\n",
    "\n",
    "To avoid classification ties, it is advisable to select $k$ appropriately. For binary classification, this typically means choosing an odd $k$. Additionally, to enhance model flexibility, a weighted version of KNN can be employed, where the influence of each of the $k$ nearest neighbors is weighted inversely by their distance to the test point. We will tune these three parameters below.\n",
    "\n",
    "### Model Workflow"
   ],
   "id": "765c55c9-adfe-442f-bb4e-4e5fab2a3b21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data already computed in the index.qmd file.\n",
    "data_split <- readRDS(here::here('data/data_split.rds'))\n",
    "data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))\n",
    "test_outcome <- readRDS(here::here('data/test_outcome.rds'))\n",
    "data_rec <- readRDS(here::here('data/data_rec.rds'))\n",
    "\n",
    "# Set random seed.\n",
    "set.seed(3145)\n"
   ],
   "id": "6734045e-eef9-4e39-a142-1bd285bd321b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a KNN model specification and workflow indicating the model hyperparameters: a number of neighbors (i.e., $k$), a weight function, and a distance function. To optimize our model, we will use the `tune::tune()` function to find optimal values of these parameters based on model accuracy."
   ],
   "id": "d9ddb95e-0ca5-4ab7-bc4c-ab004734e4a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification.\n",
    "knn_model_spec <-\n",
    "  parsnip::nearest_neighbor(\n",
    "    neighbors = tune::tune(),\n",
    "    dist_power = tune::tune(),\n",
    "    weight_func = tune::tune()\n",
    "  ) |>\n",
    "  parsnip::set_mode('classification') |>\n",
    "  parsnip::set_engine('kknn')\n",
    "\n",
    "# Create model workflow.\n",
    "knn_workflow <- workflows::workflow() |>\n",
    "  workflows::add_model(knn_model_spec) |>\n",
    "  workflows::add_recipe(data_rec)\n"
   ],
   "id": "a7abd8f0-fe2d-4661-ae5f-36c1e6b27db2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning and Fitting\n",
    "\n",
    "Next, we run our prepared workflow. To speed up the computation, we utilize parallel computing, distributing the tasks across multiple cores.\n",
    "\n",
    "We fine-tune the model hyperparameters (namely $k$, the distance function, and the weight function) using the $10$-fold cross-validation setup. We then select the best model based on accuracy."
   ],
   "id": "1afe7c34-7f15-4e45-841f-d63bb07dfccc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.923 sec elapsed"
     ]
    }
   ],
   "source": [
    "#' Check number of available cores.\n",
    "cores_no <- parallel::detectCores() - 1\n",
    "\n",
    "#' Start timer.\n",
    "tictoc::tic()\n",
    "\n",
    "# Create and register clusters.\n",
    "clusters <- parallel::makeCluster(cores_no)\n",
    "doParallel::registerDoParallel(clusters)\n",
    "\n",
    "# Fine-tune the model params.\n",
    "knn_res <- tune::tune_grid(\n",
    "  object = knn_workflow,\n",
    "  resamples = data_cross_val,\n",
    "  control = tune::control_resamples(save_pred = TRUE)\n",
    ")\n",
    "\n",
    "# Select the best fit based on accuracy.\n",
    "knn_best_fit <- \n",
    "  knn_res |> \n",
    "  tune::select_best(metric = 'accuracy')\n",
    "\n",
    "# Finalize the workflow with the best parameters.\n",
    "knn_final_workflow <- \n",
    "  knn_workflow |>\n",
    "  tune::finalize_workflow(knn_best_fit)\n",
    "\n",
    "# Fit the final model using the best parameters.\n",
    "knn_final_fit <- \n",
    "  knn_final_workflow |> \n",
    "  tune::last_fit(data_split)\n",
    "\n",
    "# Stop clusters.\n",
    "parallel::stopCluster(clusters)\n",
    "\n",
    "# Stop timer.\n",
    "tictoc::toc()\n"
   ],
   "id": "082c8cfe-c1ed-4e5c-8f21-e54e975c4c85"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "We then apply our selected model to the test set. The final metrics are given in @tbl-knn-performance-html."
   ],
   "id": "f70b30a7-5489-4756-b867-e0cb81421e4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best fit to make predictions on the test data.\n",
    "knn_pred <- \n",
    "  knn_final_fit |> \n",
    "  tune::collect_predictions() |>\n",
    "  dplyr::mutate(truth = factor(.pred_class))\n"
   ],
   "id": "99c21060-2f1e-4343-aec1-d5f3f1cde76e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table.\n",
    "knn_metrics_table <- list(\n",
    "  'Accuracy' = yardstick::accuracy_vec(truth = knn_pred[['.pred_class']],\n",
    "                                       estimate = test_outcome),\n",
    "  'Precision' = yardstick::precision_vec(truth = knn_pred[['.pred_class']],\n",
    "                                         estimate = test_outcome),\n",
    "  'Recall' = yardstick::recall_vec(truth = knn_pred[['.pred_class']],\n",
    "                                   estimate = test_outcome),\n",
    "  'Specificity' = yardstick::specificity_vec(truth = knn_pred[['.pred_class']],\n",
    "                                            estimate = test_outcome)\n",
    ") |>\n",
    "  dplyr::bind_cols() |>\n",
    "  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>\n",
    "  dplyr::mutate(Value = round(Value*100, 1))\n",
    "\n",
    "readr::write_csv(x = knn_metrics_table, file = here::here('data', 'knn-metrics.csv'))\n"
   ],
   "id": "1b53ba6c-c261-401c-9d06-b32fb261432d"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
