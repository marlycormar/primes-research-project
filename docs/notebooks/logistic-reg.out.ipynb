{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "### Model Description\n",
    "\n",
    "Logistic Regression (LR) is a supervised learning algorithm widely used for classification problems. It is particularly effective for binary classification tasks, where the outcome variable can take one of two possible values. The model predicts the probability that a given input belongs to a specific class by applying the logistic (sigmoid) function, which transforms a linear combination of input features into a probability value between $0$ and $1$.\n",
    "\n",
    "For binary classification, the logistic function is defined as $\\sigma(\\hat{Y}_i) = 1/(1 + e^{-\\hat{Y}_i})$ where $\\hat{Y}$ is a linear combination of the input features. The probability of the outcome $i$ being the positive class (represented as 1) is given by:\n",
    "\n",
    "$$\\sigma(\\hat{Y}_i) = \\sigma(\\beta_0 + \\beta_1 X_{1, i} + \\beta_2 X_{2, i} + \\ldots + \\beta_p X_{p, i}),$$\n",
    "\n",
    "where $\\beta_0$ is the intercept, and $\\beta_1, \\beta_2, \\ldots, \\beta_p$ are the coefficients corresponding to the input features $X_1, X_2, \\ldots, X_p$. These coefficients are estimated using the method of maximum likelihood estimation (MLE), which maximizes the likelihood of the observed data.\n",
    "\n",
    "LR can also be extended to handle multi-class classification problems through multinomial logistic regression. In this case, the model uses the softmax function to generalize to multiple classes. The softmax function is an extension of the logistic function for multiple classes and is defined as,\n",
    "\n",
    "$$Pr(Y = j | X = x_0) = \\frac{e^{\\mathbf{\\beta}_j x_0}}{\\sum_{i=1}^{n} e^{\\mathbf{\\beta}_i \\cdot x_0}}$$\n",
    "\n",
    "where $x_0$ is an observation, $n$ is the number of classes, and $\\mathbf{\\beta}_j$ is the coefficient vector for class $j$.\n",
    "\n",
    "The primary advantage of LR is its interpretability. Each coefficient indicates the change in the log-odds of the outcome for a one-unit change in the corresponding predictor variable. This provides clear insights into the influence of each predictor on the probability of the outcome. Despite its simplicity, LR is a powerful tool for both binary and multi-class classification, making it suitable for a wide range of applications where the relationship between the predictors and the log-odds is approximately linear.\n",
    "\n",
    "### Model Workflow"
   ],
   "id": "9b701895-3876-41c1-a9ec-f27bb5d17165"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data already computed in the index.qmd file.\n",
    "data_split <- readRDS(here::here('data/data_split.rds'))\n",
    "data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))\n",
    "test_outcome <- readRDS(here::here('data/test_outcome.rds'))\n",
    "data_rec <- readRDS(here::here('data/data_rec.rds'))\n",
    "\n",
    "# Set random seed.\n",
    "set.seed(3145)\n"
   ],
   "id": "74e374b4-c08e-4772-9350-c67655793803"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train our LR model and find the optimal values for the model parameters. The key parameter we will optimize is the penalty parameter, which refers to the regularization term added to the loss function to prevent overfitting. We will find the optimal penalty value to improve model performance. Additionally, we will set mixture = 1 to apply Lasso regularization, which helps in potentially removing irrelevant predictors and choosing a simpler model."
   ],
   "id": "1604a6d6-aefa-42bd-a270-abdf2ae7798e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification.\n",
    "lr_model_spec <-\n",
    "  parsnip::logistic_reg(\n",
    "    penalty = tune(),\n",
    "    mixture = 1) |>\n",
    "  parsnip::set_mode('classification') |>\n",
    "  parsnip::set_engine('glmnet')\n",
    "\n",
    "# Create model workflow.\n",
    "lr_workflow <- workflows::workflow() |>\n",
    "  workflows::add_model(lr_model_spec) |>\n",
    "  workflows::add_recipe(data_rec)\n"
   ],
   "id": "b6b4d008-98c0-4844-b2e2-3d8eda59851b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning and Fitting"
   ],
   "id": "fa2cb986-0947-4a16-8bdc-4651134e85bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.941 sec elapsed"
     ]
    }
   ],
   "source": [
    "#' Check number of available cores.\n",
    "cores_no <- parallel::detectCores() - 1\n",
    "\n",
    "#' Start timer.\n",
    "tictoc::tic()\n",
    "\n",
    "# Create and register clusters.\n",
    "clusters <- parallel::makeCluster(cores_no)\n",
    "doParallel::registerDoParallel(clusters)\n",
    "\n",
    "# Fine-tune the model params.\n",
    "lr_res <- tune::tune_grid(\n",
    "  object = lr_workflow,\n",
    "  resamples = data_cross_val,\n",
    "  control = tune::control_resamples(save_pred = TRUE)\n",
    ")\n",
    "\n",
    "# Select the best fit based on accuracy.\n",
    "lr_best_fit <- \n",
    "  lr_res |> \n",
    "  tune::select_best(metric = 'accuracy')\n",
    "\n",
    "# Finalize the workflow with the best parameters.\n",
    "lr_final_workflow <- \n",
    "  lr_workflow |>\n",
    "  tune::finalize_workflow(lr_best_fit)\n",
    "\n",
    "# Fit the final model using the best parameters.\n",
    "lr_final_fit <- \n",
    "  lr_final_workflow |> \n",
    "  tune::last_fit(data_split)\n",
    "\n",
    "# Stop clusters.\n",
    "parallel::stopCluster(clusters)\n",
    "\n",
    "# Stop timer.\n",
    "tictoc::toc()\n"
   ],
   "id": "6d5d845f-4e97-427e-ad41-3d1065cd4395"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "We then apply our selected model to the test set. The final metrics are given in @tbl-lr-performance-html."
   ],
   "id": "897dd25d-986a-4931-9c7b-fab13f555ef9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best fit to make predictions on the test data.\n",
    "lr_pred <- \n",
    "  lr_final_fit |> \n",
    "  tune::collect_predictions() |>\n",
    "  dplyr::mutate(truth = factor(.pred_class))\n"
   ],
   "id": "768d0e9c-a5d5-4204-9ff6-5fe22e43c28d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table.\n",
    "lr_metrics_table <- list(\n",
    "  'Accuracy' = yardstick::accuracy_vec(truth = lr_pred[['.pred_class']],\n",
    "                                       estimate = test_outcome),\n",
    "  'Precision' = yardstick::precision_vec(truth = lr_pred[['.pred_class']],\n",
    "                                         estimate = test_outcome),\n",
    "  'Recall' = yardstick::recall_vec(truth = lr_pred[['.pred_class']],\n",
    "                                   estimate = test_outcome),\n",
    "  'Specificity' = yardstick::specificity_vec(truth = lr_pred[['.pred_class']],\n",
    "                                            estimate = test_outcome)\n",
    ") |>\n",
    "  dplyr::bind_cols() |>\n",
    "  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>\n",
    "  dplyr::mutate(Value = round(Value*100, 1))\n",
    "\n",
    "readr::write_csv(x = lr_metrics_table, file = here::here('data', 'lr-metrics.csv'))\n"
   ],
   "id": "3ed33e62-433c-4965-813c-799d8c8e8dec"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
