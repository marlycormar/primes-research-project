{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# Notation\n",
    "\n",
    "Suppose we have a quantitative response $Y$ and $p$ different predictors $X_1, X_2, \\dots, X_p$. We assume that there is some relationship between $Y$ and $X = (X_1, X_2, \\dots, X_p)$, which can be written in the general form $$Y = f(X) + \\epsilon.$$ Here $f$ is some fixed but unknown function of $X_1, X_2, \\dots, X_p$ and $\\epsilon$ is a random error term, which is independent of $X$ and has mean zero. Our models will not be concerned with the form of $f$ but rather will try to formulate an estimate $\\hat{f}$ of $f$ that in turn produces an estimate $\\hat{Y}$ of $Y$. The formulation of interest becomes $$\\hat{Y} = \\hat{f}(X),$$ where $\\hat{f}$ is treated as a black box and the mean-zero error $\\epsilon$ is dropped.\n",
    "\n",
    "Suppose the outcome is the set with classes $1, 2, \\dots, n$. The Bayes Classifier assigns each observation to the most likely class, given its predictor values. In other words, we assign class $j \\in \\{1, 2, \\dots, n\\}$ to the test observation $x_0$ if $$Pr(Y = j | X = x_0) = \\max_{i} Pr(Y = i | X = x_0)$$\n",
    "\n",
    "The Bayes classifier produces the lowest possible test error rate called the Bayes error rate. In other words, the Bayes classifier minimizes the test error defined by $$\\frac{1}{2} \\sum_{i = 1}^{n} I(y_i \\neq \\hat{y}_i),$$\n",
    "\n",
    "where $I(y_i \\neq \\hat{y}_i)$ is an indicator variable that equals $1$ if $y_i \\neq \\hat{y}_i$ and zero if $y_i = \\hat{y}_i$ (i.e., the $i$th observation was classified correctly). The Bayes decision boundary is determined by those observations which conditional probability is exactly $0.5$."
   ],
   "id": "43754d40-b3c2-48ac-a7b4-6c593c4c985c"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
