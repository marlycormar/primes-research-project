{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis of Machine Learning Models for Thyroid Cancer\n",
    "\n",
    "Recurrence Prediction\n",
    "\n",
    "Anay Aggarwal (MIT PRIMES)  \n",
    "Ekam Kaur (MIT PRIMES)  \n",
    "Susie Lu (MIT PRIMES)  \n",
    "June 20, 2024\n",
    "\n",
    "This study assesses the efficacy of machine learning algorithms—Artificial Neural Network, K-Nearest Neighbors, Support Vector Machine, Logistic Regression, Extreme Gradient Boosting, and Random Forest—in predicting recurrence in Differentiated Thyroid Cancer (DTC). Using a comprehensive dataset from the UCI Machine Learning Repository, we conduct a comparative analysis of these algorithms based on key performance metrics. Our investigation seeks to identify the best model for predicting DTC recurrence, aiming to advance personalized treatment strategies and improve clinical outcomes in thyroid cancer care."
   ],
   "id": "4476fe22-d2a1-413a-8ef1-c82d7eda480d"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "c4715f22-b3e9-40c6-8d9c-d71f84ec2f2c"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!---------------------------- ## INTRODUCTION -------------------------------->"
   ],
   "id": "04dc1f01-5014-4afe-bd28-ef79db0cc3e5"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "8168907c-2e7e-4cdc-a6ba-fcef4b4de7e0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Thyroid cancer is among the most prevalent endocrine malignancies worldwide, with differentiated thyroid cancer (DTC) representing the majority of cases \\[[1](#ref-pellegriti2013)\\]. The management and prognosis of thyroid cancer critically depend on the timely and accurate prediction of cancer recurrence. Traditional approaches, primarily based on clinical and pathological parameters, often lack the precision required for personalized treatment planning \\[[2](#ref-bhattacharya2023)\\]. With advancements in machine learning (ML) techniques, there’s a burgeoning interest in leveraging these analytical tools to enhance the predictive accuracy regarding cancer recurrence, thereby improving the efficacy of therapeutic interventions and follow-up strategies \\[[3](#ref-xi2022)\\].\n",
    "\n",
    "This paper explores the application of six distinct machine learning algorithms—Artificial Neural Network (ANN), Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Logistic Regression (LR), and the ensemble learning methods – Random Forest (RF) and Extreme Gradient Boosting (XGBoost) — to predict cancer recurrence in patients with differentiated thyroid cancer. Utilizing the differentiated thyroid cancer recurrence dataset from the UCI Machine Learning Repository \\[[4](#ref-borzooei2023)\\], this study aims to compare the predictive performance of these algorithms in a clinical setting. The dataset comprises patient demographic information, clinical features, and pathological details, providing a solid foundation for predictive modeling.\n",
    "\n",
    "ANNs, celebrated for their capability to model complex nonlinear relationships through interconnected processing elements, offer powerful tools for medical prediction tasks \\[[5](#ref-abiodun2018)\\]. SVMs, recognized for their effectiveness in high-dimensional spaces and versatility in handling both linear and nonlinear data, present a robust methodology for classification challenges \\[[6](#ref-cervantes2020)\\]. KNN, a straightforward yet efficient algorithm based on feature space proximity, offers an intuitive approach to classification by leveraging the similarity between cases \\[[7](#ref-syriopoulos2022)\\]. LR, known for its simplicity and efficient training, offers a intuitive way for determining the decision boundary between linearly separable data points, and directly outputs the predicted probabilities for each class \\[[8](#ref-maalouf2011)\\]. RF, an ensemble learning method that uses bagging on classification trees, proves to be efficient and accurate in handling high-dimensional and non-linear data \\[[9](#ref-genuer2010)\\]. XGBoost, an ensemble learning method that uses boosting (meaning that the algorithm sequentially generates new trees based on previous training output), is very fast and robust for a variety of tasks \\[[10](#ref-anju2018)\\].\n",
    "\n",
    "By applying all six machine learning approaches to the differentiated thyroid cancer recurrence dataset, this study seeks to identify the most effective model for predicting cancer recurrence, thus contributing to the ongoing efforts to enhance patient outcomes in thyroid cancer management.\n",
    "\n",
    "Through a rigorous evaluation of model performance based on accuracy, precision, recall, and specificity, this paper aims to shed light on the strengths and limitations of each algorithm in the context of thyroid cancer recurrence prediction. Furthermore, the study discusses the implications of these findings for clinical practice, emphasizing the potential of machine learning to revolutionize cancer care by enabling more accurate and personalized risk assessments \\[[11](#ref-konstantina2015)\\]"
   ],
   "id": "9de3d4a2-149e-4de1-a03e-0e32929f9485"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "63cee3b8-95f8-4514-91f1-c125b43003a4"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!------------------------------ ## NOTATION ---------------------------------->"
   ],
   "id": "f4583fd3-686f-44bb-9e25-cc6dee2d45ee"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "b805793e-3897-45c6-9125-2de40944ad17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Notation\n",
    "\n",
    "Suppose we have a quantitative response $Y$ and $p$ different predictors $X_1, X_2, \\dots, X_p$. We assume that there is some relationship between $Y$ and $X = (X_1, X_2, \\dots, X_p)$, which can be written in the general form $$Y = f(X) + \\epsilon.$$ Here $f$ is some fixed but unknown function of $X_1, X_2, \\dots, X_p$ and $\\epsilon$ is a random error term, which is independent of $X$ and has mean zero. Our models will not be concerned with the form of $f$ but rather will try to formulate an estimate $\\hat{f}$ of $f$ that in turn produces an estimate $\\hat{Y}$ of $Y$. The formulation of interest becomes $$\\hat{Y} = \\hat{f}(X),$$ where $\\hat{f}$ is treated as a black box and the mean-zero error $\\epsilon$ is dropped.\n",
    "\n",
    "Suppose the outcome is the set with classes $1, 2, \\dots, n$. The Bayes Classifier assigns each observation to the most likely class, given its predictor values. In other words, we assign class $j \\in \\{1, 2, \\dots, n\\}$ to the test observation $x_0$ if $$Pr(Y = j | X = x_0) = \\max_{i} Pr(Y = i | X = x_0)$$\n",
    "\n",
    "The Bayes classifier produces the lowest possible test error rate called the Bayes error rate. In other words, the Bayes classifier minimizes the test error defined by $$\\frac{1}{2} \\sum_{i = 1}^{n} I(y_i \\neq \\hat{y}_i),$$\n",
    "\n",
    "where $I(y_i \\neq \\hat{y}_i)$ is an indicator variable that equals $1$ if $y_i \\neq \\hat{y}_i$ and zero if $y_i = \\hat{y}_i$ (i.e., the $i$th observation was classified correctly). The Bayes decision boundary is determined by those observations which conditional probability is exactly $0.5$."
   ],
   "id": "6b210b54-ff6b-4bce-b0bf-6dcd510824ec"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "be9aa94e-dd14-4855-9a6f-99de6d146637"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--------------------------- ## DATA & METHODS ------------------------------->"
   ],
   "id": "3c540ed8-5efa-41f8-b840-9479d40d4e4c"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "02a230ed-88cb-4ff6-9b16-eeed54759029"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- EDA --->"
   ],
   "id": "d25ce4d6-c4b3-4016-9d6d-5be4785b4fd6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analysis\n",
    "\n",
    "Our study focuses on the “Differentiated Thyroid Cancer Recurrence” dataset \\[[4](#ref-borzooei2023)\\] hosted by the UCI Machine Learning Repository. The UCI Machine Learning Repository offers a wide array of datasets used for empirical analysis in machine learning and data mining \\[[12](#ref-ucidata)\\]. Established by the University of California, Irvine, this repository facilitates academic and educational pursuits by providing free access to datasets that cover various domains. As of March, 2024, it hosts and maintains over 600 datasets."
   ],
   "id": "d181284b-6cb8-4df4-b2f9-fa768b1a316c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Rows: 383 Columns: 17\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (16): Gender, Smoking, Hx Smoking, Hx Radiothreapy, Thyroid Function, Ph...\n",
      "dbl  (1): Age\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
     ]
    }
   ],
   "source": [
    "raw_data <- readr::read_csv(here::here('data/raw-data.csv'))"
   ],
   "id": "9a56baa7-61a5-4ced-aa90-cdcfed909cfb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “Differentiated Thyroid Cancer Recurrence” dataset encompasses 383 samples or observations, and a range of 17 variables pertinent to thyroid cancer, including patient demographics, clinical features, and pathological details, all aimed at elucidating patterns associated with cancer recurrence.\n",
    "\n",
    "We will employ six distinct modeling methods to analyze our dataset: Artificial Neural Network (ANN), K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), and Extreme Gradient Boosting (XGBoost). Each of these methods brings unique strengths to the analysis, with ANN providing deep learning capabilities, KNN offering simplicity and ease of interpretation, SVM delivering powerful discriminative classification, LR providing an intuitive and easily trainable implementation, and the ensemble methods RF and XGBoost offering highly robust and accurate tree algorithms – thereby encompassing a comprehensive approach to predicting cancer recurrence in the studied dataset.\n",
    "\n",
    "To prepare our data for modeling, we fix a typographical error, remove duplicate observations, and transform categorical variables into factors."
   ],
   "id": "3eb51b8c-9712-45af-b8e7-ccd3dc6fb283"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Rows: 383 Columns: 17\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (16): Gender, Smoking, Hx Smoking, Hx Radiothreapy, Thyroid Function, Ph...\n",
      "dbl  (1): Age\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
     ]
    }
   ],
   "source": [
    "#' Load raw data.\n",
    "cleaned_data <- \n",
    "  readr::read_csv(here::here('data/raw-data.csv')) |>\n",
    "  dplyr::distinct() |>\n",
    "  dplyr::rename(`Hx Radiotherapy` = 'Hx Radiothreapy') |>\n",
    "  dplyr::mutate(Gender = ifelse(Gender == 'F', 'Female', 'Male')) |>\n",
    "  dplyr::mutate(\n",
    "    Gender = factor(Gender, levels = c('Female', 'Male')),\n",
    "    Smoking = factor(Smoking, levels = c('Yes', 'No')),\n",
    "    `Hx Smoking` = factor(`Hx Smoking`, levels = c('Yes', 'No')),\n",
    "    `Hx Radiotherapy` = factor(`Hx Radiotherapy`, levels = c('Yes', 'No')),\n",
    "    `Thyroid Function` = factor(\n",
    "      `Thyroid Function`,\n",
    "      levels = c('Euthyroid', 'Clinical Hyperthyroidism',\n",
    "                 'Subclinical Hyperthyroidism', 'Clinical Hypothyroidism',\n",
    "                 'Subclinical Hypothyroidism')),\n",
    "    `Physical Examination` = factor(`Physical Examination`,\n",
    "                                    levels = c('Normal', 'Diffuse goiter', \n",
    "                                               'Single nodular goiter-right',\n",
    "                                               'Single nodular goiter-left', \n",
    "                                               'Multinodular goiter')),\n",
    "    Adenopathy = factor(Adenopathy,\n",
    "                        levels = c('No', 'Right', 'Left', 'Bilateral', \n",
    "                                   'Posterior', 'Extensive')),\n",
    "    Pathology = factor(\n",
    "      Pathology,\n",
    "      levels = c('Papillary', 'Micropapillary', 'Follicular',\n",
    "                 'Hurthel cell')),\n",
    "    Focality = factor(Focality, levels = c('Uni-Focal', 'Multi-Focal')),\n",
    "    `T` = factor(`T`, levels = c('T1a', 'T1b', 'T2', 'T3a', 'T3b', 'T4a',\n",
    "                                 'T4b')),\n",
    "    N = factor(N, levels = c('N0', 'N1b', 'N1a')),\n",
    "    M = factor(M, levels = c('M0', 'M1')),\n",
    "    Stage = factor(Stage, levels = c('I', 'II', 'III', 'IVA', 'IVB')),\n",
    "    Response = factor(\n",
    "      Response,\n",
    "      levels = c('Excellent', 'Biochemical Incomplete',\n",
    "                 'Structural Incomplete', 'Indeterminate')),\n",
    "    Risk = factor(Risk, levels = c('Low', 'Intermediate', 'High')),\n",
    "    Recurred = factor(Recurred, levels = c('Yes', 'No'))\n",
    "  )"
   ],
   "id": "ba70e96a-71d8-4bb8-a57a-dedb23ac807a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data.\n",
    "readr::write_csv(x = cleaned_data, file = here::here('data', 'cleaned-data.csv'))"
   ],
   "id": "35046d0c-fb50-4dba-98bb-70defd16ff2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total observations.\n",
    "total_obs <- nrow(cleaned_data)\n",
    "\n",
    "# Number of males/females & gender percentages.\n",
    "fem_no <- sum(cleaned_data$Gender == 'Female')\n",
    "males_no <- sum(cleaned_data$Gender == 'Male')\n",
    "fem_perc <- round(fem_no/total_obs*100, 1)\n",
    "males_perc <- round(males_no/total_obs*100, 1)\n",
    "\n",
    "# Gender by recurrence.\n",
    "fem_rec_yes <- sum(cleaned_data$Gender == 'Female' & cleaned_data$Recurred == 'Yes')\n",
    "fem_rec_no <- sum(cleaned_data$Gender == 'Female' & cleaned_data$Recurred == 'No')\n",
    "male_rec_yes <- sum(cleaned_data$Gender == 'Male' & cleaned_data$Recurred == 'Yes')\n",
    "male_rec_no <- sum(cleaned_data$Gender == 'Male' & cleaned_data$Recurred == 'No')\n",
    "fem_rec_yes_perc <- round(fem_rec_yes/fem_no*100, 1)\n",
    "fem_rec_no_perc <- round(fem_rec_no/fem_no*100, 1)\n",
    "male_rec_yes_perc <- round(male_rec_yes/males_no*100, 1)\n",
    "male_rec_no_perc <- round(male_rec_no/males_no*100, 1)"
   ],
   "id": "28dcf83d-2702-454c-886d-61a900edfe71"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing duplicates, our data has 364 observations. Out of the 17 variables, 16 will be used as features, leaving `Recurred` as the target variable to be predicted. Among the patients, there is a significant disparity between males and females: 293(80.5%) are females and 71(19.5%) are males. Males are about evenly distributed in terms of cancer recurrence with 59.2% total recurred cases. On the other hand, females are not evenly distributed in terms of cancer recurrence with 22.5% total recurred cases (see <a href=\"#fig-gender-dist-html\" class=\"quarto-xref\">Figure 1</a> )."
   ],
   "id": "126bd470-dec4-420f-92a7-650d156c1427"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this plot will only show for PDF versions of the paper.\n",
    "knitr::include_graphics(here::here('images/gender_dist_plot.png'))"
   ],
   "id": "cell-fig-gender-dist-pdf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n",
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n",
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels"
     ]
    }
   ],
   "source": [
    "# Note: this plot will only show for HTML versions of the paper.\n",
    "\n",
    "# Gender distribution grouped by cancer recurrence.\n",
    "gender_dist_plot <- cleaned_data |>\n",
    "  dplyr::mutate(fem_total = sum(Gender == 'Female'),\n",
    "                male_total= sum(Gender == 'Male')) |>\n",
    "  dplyr::group_by(Gender, Recurred) |>\n",
    "  dplyr::reframe(count = dplyr::n(), fem_total, male_total) |>\n",
    "  dplyr::mutate(\n",
    "    count = ifelse(Gender == 'Female',\n",
    "                   round(count/fem_total*100, 1),\n",
    "                   round(count/male_total*100, 1))) |>\n",
    "  dplyr::distinct() |>\n",
    "  plotly::plot_ly(\n",
    "    x = ~Gender,\n",
    "    y = ~count,\n",
    "    color = ~Recurred,\n",
    "    text = ~Recurred,\n",
    "    opacity = 0.7,\n",
    "    type = 'bar',\n",
    "    hovertemplate = '<b>Gender</b>: %{x} <br><b>Recurred</b>: %{text} <br><b>Percentage</b>: %{y} <extra></extra>'\n",
    "    ) |>\n",
    "  plotly::config(displayModeBar = FALSE) |>\n",
    "  plotly::layout(bargap = 0.5, barmode = 'stack',\n",
    "                 yaxis = list(title = '', ticksuffix = '%'),\n",
    "                 legend = list(title = list(text = '<b>Recurred</b>'))\n",
    "  )\n",
    "\n",
    "plotly::save_image(gender_dist_plot, here::here('images/gender_dist_plot.png'),\n",
    "                   width = 500, scale = 4)"
   ],
   "id": "cell-fig-gender-dist-html"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `Age` by cancer recurrence is shown in <a href=\"#fig-age-dist-html\" class=\"quarto-xref\">Figure 2</a>. Note that, in general, older patients are more likely to recur."
   ],
   "id": "42b78d0a-1b35-4154-b7cf-6fb4f79f3b2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this plot will only show for PDF versions of the paper.\n",
    "knitr::include_graphics(here::here('images/age_dist_plot.png'))"
   ],
   "id": "cell-fig-age-dist-pdf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n",
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n",
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels"
     ]
    }
   ],
   "source": [
    "# Note: this plot will only show for HTML versions of the paper.\n",
    "\n",
    "#' Age Distribution grouped by cancer recurrence.\n",
    "age_dist_plot <- cleaned_data |>\n",
    "  plotly::plot_ly() |>\n",
    "  plotly::add_trace(\n",
    "    x = ~Age,\n",
    "    color = ~Recurred,\n",
    "    text = ~Recurred,\n",
    "    opacity = 0.7, #marker = list(color = '02d46a'),\n",
    "    type = 'histogram',\n",
    "    histnorm = 'percent',\n",
    "    hovertemplate = '<b>Age Range</b>: %{x} years <br><b>Percentage</b>: %{y:.1f}%<br><b>Recurred</b>: %{text}<extra></extra>'\n",
    "    ) |>\n",
    "  plotly::config(displayModeBar = FALSE) |>\n",
    "  plotly::layout(bargap = 0.1, barmode = 'stack',\n",
    "                 yaxis = list(ticksuffix = '%'),\n",
    "                 legend = list(title = list(text = '<b>Recurred</b>'))\n",
    "  )\n",
    "\n",
    "plotly::save_image(age_dist_plot, here::here('images/age_dist_plot.png'), scale = 4)"
   ],
   "id": "cell-fig-age-dist-html"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of the data features to be shown in a table.\n",
    "dt_summary <- purrr::map(\n",
    "  colnames(cleaned_data |> dplyr::select(-Age, -Recurred)),\n",
    "  \\(x) paste0(unique(sort(cleaned_data[[x]])), collapse = ', ')\n",
    ")\n",
    "names(dt_summary) <- colnames(cleaned_data |> dplyr::select(-Age, -Recurred))\n",
    "dt_summary <- tibble::as_tibble(dt_summary) |>\n",
    "  tidyr::pivot_longer(\n",
    "    cols = dplyr::everything(),\n",
    "    names_to = 'Feature',\n",
    "    values_to = 'Values'\n",
    "  )"
   ],
   "id": "960bc94a-cb7d-4e27-bc42-d4c579958dfd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides `Age`, the rest of the features are categorical. One interesting categorical feature is `Adenopathy`. It represents the presence of swollen lymph nodes during physical examination. The different adenopathy types observed are no adenopathy, anterior right, anterior left, bilateral (i.e., both sides of the body), posterior, and extensive (i.e., involves all the locations). Note the high correlation between swollen lymph nodes and DTC recurrence rate (see <a href=\"#fig-aden-dist-html\" class=\"quarto-xref\">Figure 3</a>)."
   ],
   "id": "3a93310f-7456-4c71-a9b5-e83bb43bb264"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this plot will only show for PDF versions of the paper.\n",
    "knitr::include_graphics(here::here('images/aden_dist_plot.png'))"
   ],
   "id": "cell-fig-aden-dist-pdf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n",
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n",
      "Warning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels"
     ]
    }
   ],
   "source": [
    "# Note: this plot will only show for HTML versions of the paper.\n",
    "\n",
    "# Adenopathy distribution by cancer recurrence.\n",
    "aden_dist_plot <- cleaned_data |>\n",
    "  #' Find first the total number of patients by Adenopathy.\n",
    "  #' This will be used to find the percentage of repeating cases next.\n",
    "  dplyr::reframe(\n",
    "    total_aden = dplyr::n(),\n",
    "    .by = Adenopathy,\n",
    "    Recurred # Keep this column.\n",
    "  ) |>\n",
    "  # Calculate the number of cancer recurrences per adepathy. Calculate %.\n",
    "  dplyr::reframe(\n",
    "    total_rec = round(dplyr::n()/total_aden*100, 1),\n",
    "    .by = c(Adenopathy, Recurred),\n",
    "    total_aden = total_aden\n",
    "  ) |>\n",
    "  dplyr::distinct() |>\n",
    "  plotly::plot_ly(\n",
    "    x = ~Adenopathy,\n",
    "    y = ~total_rec,\n",
    "    color = ~Recurred,\n",
    "    text = ~Recurred,\n",
    "    opacity = 0.7,\n",
    "    type = 'bar',\n",
    "    hovertext = ~total_aden,\n",
    "    hovertemplate = '<b>Adenopathy</b>: %{x} <br><b>Recurred</b>: %{text} <br><b>Percentage</b>: %{y} of %{hovertext} patients <extra></extra>'\n",
    "    ) |>\n",
    "  plotly::config(displayModeBar = FALSE) |>\n",
    "  plotly::layout(bargap = 0.1, barmode = 'stack',\n",
    "                 yaxis = list(title = '', ticksuffix = '%'),\n",
    "                 legend = list(title = list(text = '<b>Recurred</b>'))\n",
    "  )\n",
    "\n",
    "plotly::save_image(aden_dist_plot, here::here('images/aden_dist_plot.png'), scale = 4)"
   ],
   "id": "cell-fig-aden-dist-html"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of all the features and their categories are shown in **?@tbl-summary-html**."
   ],
   "id": "2db1e6f6-28b4-44da-bd0f-b44349d12d53"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "daef53a2-f9df-41eb-97cd-28e85145f7e6"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--------------------------------- ## MODELS --------------------------------->"
   ],
   "id": "0a8cfa9d-abdf-489c-a699-fd050467513d"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "5e1fc767-ad71-46bf-981b-1eb31e0845ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training\n",
    "\n",
    "Let us split the cleaned data set into a training ($75\\%$) set and a test ($25\\%$) set using a random generator. The training data will be further separated into 10 folds for cross-validation."
   ],
   "id": "bdc6d776-f3c7-48ee-970a-5abd6b00aba7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets.\n",
    "set.seed(314)\n",
    "data_split <- rsample::initial_split(cleaned_data)\n",
    "train_data <- rsample::training(data_split)\n",
    "test_data <- rsample::testing(data_split)\n",
    "\n",
    "# Split the training data into 10-folds for cross-validation.\n",
    "set.seed(3145)\n",
    "data_cross_val <- rsample::vfold_cv(train_data)\n",
    "\n",
    "# Set aside the outcome column of the sample test data.\n",
    "test_outcome <- factor(test_data$Recurred)"
   ],
   "id": "700069de-815b-4353-a8b3-65b7ca6f4fce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `recipes` package is useful to create a blueprint of the pre-processing steps that will be applied to our data during model training. We use this package to specify that\n",
    "\n",
    "-   the minimum number of features with absolute correlations less that $0.6$ should be removed,\n",
    "-   the numeric features should be normalized, and\n",
    "-   the categorical variables should be transformed into numerical variables."
   ],
   "id": "a578351b-1413-498a-b16a-1a58b61d31ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recipe for the data prep.\n",
    "data_rec <- recipes::recipe(Recurred ~ ., data = train_data) |>\n",
    "  recipes::step_corr(threshold = 0.6) |>\n",
    "  recipes::step_normalize(recipes::all_numeric()) |>\n",
    "  recipes::step_dummy(recipes::all_nominal_predictors())"
   ],
   "id": "7e689f0c-05fc-4118-a67c-a7980ae3cff2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Save split data and the recipes to be used by the notebooks scripts.\n",
    "#' This is important to avoid differences in the scripts and remove duplicate\n",
    "#' computations.\n",
    "#' Note that this is not shown in the paper.\n",
    "saveRDS(data_split, 'data/data_split.rds')\n",
    "saveRDS(data_cross_val, 'data/data_cross_val.rds')\n",
    "saveRDS(test_outcome, 'data/test_outcome.rds')\n",
    "saveRDS(data_rec, 'data/data_rec.rds')"
   ],
   "id": "7db62aa7-846d-424f-8661-7aac3039d21f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 K-Nearest Neighbors\n",
    "\n",
    "### 4.1.1 Model Description\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a nonparametric method used for classification. It classifies a given sample based on the proximity to the training data. The algorithm determines the class of a point $X$ by identifying the most common class label among its $k$ nearest neighbors, where $k$ is a predetermined hyperparameter. Unlike other algorithms, the KNN classifier does not involve training a model; instead, it memorizes the training data, making it a “lazy” algorithm.\n",
    "\n",
    "The primary hyperparameters of the KNN algorithm are $k$, the distance measure, and the weight function. Common distance measures include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "\n",
    "Choosing the optimal value for $k$ is crucial and involves balancing the bias-variance tradeoff. A small $k$ results in low bias and high variance. Low bias means the model captures the complexity of the training data very well, but high variance means the model is highly sensitive to the specifics of the training data, often leading to overfitting and higher test errors. As $k$ increases, the model averages over more neighbors, which smooths out the predictions and reduces the model’s sensitivity to individual data points, thus reducing variance. Therefore, a large $k$ results in high bias and low variance. The model may become too simplistic, leading to higher bias, but it becomes less sensitive to the training data, making it more robust to noise and better at generalizing to new data.\n",
    "\n",
    "To avoid classification ties, it is advisable to select $k$ appropriately. For binary classification, this typically means choosing an odd $k$. Additionally, to enhance model flexibility, a weighted version of KNN can be employed, where the influence of each of the $k$ nearest neighbors is weighted inversely by their distance to the test point. We will tune these three parameters below.\n",
    "\n",
    "### 4.1.2 Model Workflow"
   ],
   "id": "b667de87-a09c-467d-84d4-1690b7b052ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data already computed in the index.qmd file.\n",
    "data_split <- readRDS(here::here('data/data_split.rds'))\n",
    "data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))\n",
    "test_outcome <- readRDS(here::here('data/test_outcome.rds'))\n",
    "data_rec <- readRDS(here::here('data/data_rec.rds'))\n",
    "\n",
    "# Set random seed.\n",
    "set.seed(3145)"
   ],
   "id": "b1a6c19d-2a00-4178-a733-e2e5c0f4940e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a KNN model specification and workflow indicating the model hyperparameters: a number of neighbors (i.e., $k$), a weight function, and a distance function. To optimize our model, we will use the `tune::tune()` function to find optimal values of these parameters based on model accuracy."
   ],
   "id": "5782ec18-38db-416e-ba86-b563e609b0f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification.\n",
    "knn_model_spec <-\n",
    "  parsnip::nearest_neighbor(\n",
    "    neighbors = tune::tune(),\n",
    "    dist_power = tune::tune(),\n",
    "    weight_func = tune::tune()\n",
    "  ) |>\n",
    "  parsnip::set_mode('classification') |>\n",
    "  parsnip::set_engine('kknn')\n",
    "\n",
    "# Create model workflow.\n",
    "knn_workflow <- workflows::workflow() |>\n",
    "  workflows::add_model(knn_model_spec) |>\n",
    "  workflows::add_recipe(data_rec)"
   ],
   "id": "5924803a-d6f0-4e94-bc88-ba45ea1f6eff"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Model Tuning and Fitting\n",
    "\n",
    "Next, we run our prepared workflow. To speed up the computation, we utilize parallel computing, distributing the tasks across multiple cores.\n",
    "\n",
    "We fine-tune the model hyperparameters (namely $k$, the distance function, and the weight function) using the $10$-fold cross-validation setup. We then select the best model based on accuracy."
   ],
   "id": "74b15976-31ed-4fdb-bf48-6cc5221b127c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15.362 sec elapsed"
     ]
    }
   ],
   "source": [
    "#' Check number of available cores.\n",
    "cores_no <- parallel::detectCores() - 1\n",
    "\n",
    "#' Start timer.\n",
    "tictoc::tic()\n",
    "\n",
    "# Create and register clusters.\n",
    "clusters <- parallel::makeCluster(cores_no)\n",
    "doParallel::registerDoParallel(clusters)\n",
    "\n",
    "# Fine-tune the model params.\n",
    "knn_res <- tune::tune_grid(\n",
    "  object = knn_workflow,\n",
    "  resamples = data_cross_val,\n",
    "  control = tune::control_resamples(save_pred = TRUE)\n",
    ")\n",
    "\n",
    "# Select the best fit based on accuracy.\n",
    "knn_best_fit <- \n",
    "  knn_res |> \n",
    "  tune::select_best(metric = 'accuracy')\n",
    "\n",
    "# Finalize the workflow with the best parameters.\n",
    "knn_final_workflow <- \n",
    "  knn_workflow |>\n",
    "  tune::finalize_workflow(knn_best_fit)\n",
    "\n",
    "# Fit the final model using the best parameters.\n",
    "knn_final_fit <- \n",
    "  knn_final_workflow |> \n",
    "  tune::last_fit(data_split)\n",
    "\n",
    "# Stop clusters.\n",
    "parallel::stopCluster(clusters)\n",
    "\n",
    "# Stop timer.\n",
    "tictoc::toc()"
   ],
   "id": "86a408ad-d6a2-4563-8176-87152f5895cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Model Performance\n",
    "\n",
    "We then apply our selected model to the test set. The final metrics are given in **?@tbl-knn-performance-html**."
   ],
   "id": "2487f421-7ac0-4b8c-934f-8976205067da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best fit to make predictions on the test data.\n",
    "knn_pred <- \n",
    "  knn_final_fit |> \n",
    "  tune::collect_predictions() |>\n",
    "  dplyr::mutate(truth = factor(.pred_class))"
   ],
   "id": "8251d322-0ee8-4188-bbd7-68156732f5ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table.\n",
    "knn_metrics_table <- list(\n",
    "  'Accuracy' = yardstick::accuracy_vec(truth = knn_pred[['.pred_class']],\n",
    "                                       estimate = test_outcome),\n",
    "  'Precision' = yardstick::precision_vec(truth = knn_pred[['.pred_class']],\n",
    "                                         estimate = test_outcome),\n",
    "  'Recall' = yardstick::recall_vec(truth = knn_pred[['.pred_class']],\n",
    "                                   estimate = test_outcome),\n",
    "  'Specificity' = yardstick::specificity_vec(truth = knn_pred[['.pred_class']],\n",
    "                                            estimate = test_outcome)\n",
    ") |>\n",
    "  dplyr::bind_cols() |>\n",
    "  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>\n",
    "  dplyr::mutate(Value = round(Value*100, 1))\n",
    "\n",
    "readr::write_csv(x = knn_metrics_table, file = here::here('data', 'knn-metrics.csv'))"
   ],
   "id": "71c8a211-f5e3-41a4-8ecc-431aa04b547b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Support Vector Machine\n",
    "\n",
    "### 4.2.1 Model Description\n",
    "\n",
    "Support Vector Machines (SVM) are powerful supervised learning algorithms used for both classification and regression tasks. For classification, SVM works by finding the hyperplane that best separates data points of different classes in a high-dimensional space. The optimal hyperplane is determined by maximizing the margin between the closest points of the classes, known as support vectors.\n",
    "\n",
    "SVM is particularly effective in high-dimensional spaces and is useful when the number of dimensions exceeds the number of samples. It can employ various kernel functions—such as linear, polynomial, and radial basis function (RBF)—to handle non-linear classification by mapping input features into higher-dimensional spaces.\n",
    "\n",
    "The most commonly used kernel in SVM is the Radial Basis Function (RBF) kernel, also known as the Gaussian kernel. The RBF kernel maps input features into an infinite-dimensional space, allowing SVM to create complex decision boundaries. The RBF kernel function is defined as:\n",
    "\n",
    "$$K(X_i, X_j) = e^{- \\frac{|| X_i - X_j ||^2}{2 \\sigma^2}}$$ where $X_i$ and $X_j$ are the input feature vectors, and $\\sigma$ is a parameter that determines the spread of the kernel and controls the influence of individual training samples.\n",
    "\n",
    "### 4.2.2 Model Workflow"
   ],
   "id": "c76a9e29-325b-44a5-b010-5f5a7d4c0e84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data already computed in the index.qmd file.\n",
    "data_split <- readRDS(here::here('data/data_split.rds'))\n",
    "data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))\n",
    "test_outcome <- readRDS(here::here('data/test_outcome.rds'))\n",
    "data_rec <- readRDS(here::here('data/data_rec.rds'))\n",
    "\n",
    "# Set random seed.\n",
    "set.seed(3145)"
   ],
   "id": "e3772736-98b0-4186-b5be-df3b67de91e8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an SVM model specification and workflow, indicating the model hyperparameters $\\sigma$ (or `rbf_sigma`) and `cost`. The `rbf_sigma` parameter controls the influence of individual training examples, while the `cost` parameter controls the trade-off between achieving a low training error and a low testing error, which affects the model’s ability to generalize. To optimize our model, we will use the `tune::tune()` function to find the optimal values of these parameters in terms of model accuracy."
   ],
   "id": "bf713be1-33ac-4c4c-ad42-94f13ad45145"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification.\n",
    "svm_model_spec <-\n",
    "  parsnip::svm_rbf(\n",
    "    cost = tune::tune(),\n",
    "    rbf_sigma = tune::tune()\n",
    "  ) |>\n",
    "  parsnip::set_engine('kernlab') |>\n",
    "  parsnip::set_mode('classification')\n",
    "\n",
    "# Create model workflow.\n",
    "svm_workflow <- workflows::workflow() |>\n",
    "  workflows::add_model(svm_model_spec) |>\n",
    "  workflows::add_recipe(data_rec)"
   ],
   "id": "533dfe5c-33c3-4128-9fd1-5683fdeda35d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Model Tuning and Fitting\n",
    "\n",
    "As we did for KNN, we use parallel computing to fine-tuning our model using the $10$-fold cross-validation we set up earlier. We end this section by selecting the best model based on accuracy."
   ],
   "id": "63f648f8-9445-4e54-a4ca-0cf0fbe3eedf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16.904 sec elapsed"
     ]
    }
   ],
   "source": [
    "#' Check number of available cores.\n",
    "cores_no <- parallel::detectCores() - 1\n",
    "\n",
    "#' Start timer.\n",
    "tictoc::tic()\n",
    "\n",
    "# Create and register clusters.\n",
    "clusters <- parallel::makeCluster(cores_no)\n",
    "doParallel::registerDoParallel(clusters)\n",
    "\n",
    "# Fine-tune the model params.\n",
    "svm_res <- tune::tune_grid(\n",
    "  object = svm_workflow,\n",
    "  resamples = data_cross_val,\n",
    "  control = tune::control_resamples(save_pred = TRUE)\n",
    ")\n",
    "\n",
    "# Select the best fit based on accuracy.\n",
    "svm_best_fit <- \n",
    "  svm_res |> \n",
    "  tune::select_best(metric = 'accuracy')\n",
    "\n",
    "# Finalize the workflow with the best parameters.\n",
    "svm_final_workflow <- \n",
    "  svm_workflow |>\n",
    "  tune::finalize_workflow(svm_best_fit)\n",
    "\n",
    "# Fit the final model using the best parameters.\n",
    "svm_final_fit <- \n",
    "  svm_final_workflow |> \n",
    "  tune::last_fit(data_split)\n",
    "\n",
    "# Stop clusters.\n",
    "parallel::stopCluster(clusters)\n",
    "\n",
    "# Stop timer.\n",
    "tictoc::toc()"
   ],
   "id": "69c229e2-3c4d-4882-bc29-ef655b4daf84"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Model Performance\n",
    "\n",
    "We then apply our selected model to the test set. The final metrics are given in **?@tbl-svm-performance-html**."
   ],
   "id": "8d1651a1-4e4c-4d3e-ad15-009ab145379b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best fit to make predictions on the test data.\n",
    "svm_pred <- \n",
    "  svm_final_fit |> \n",
    "  tune::collect_predictions() |>\n",
    "  dplyr::mutate(truth = factor(.pred_class))"
   ],
   "id": "9c9d61f0-7f3d-4745-8ece-11d9835b1b80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table.\n",
    "svm_metrics_table <- list(\n",
    "  'Accuracy' = yardstick::accuracy_vec(truth = svm_pred[['.pred_class']],\n",
    "                                       estimate = test_outcome),\n",
    "  'Precision' = yardstick::precision_vec(truth = svm_pred[['.pred_class']],\n",
    "                                         estimate = test_outcome),\n",
    "  'Recall' = yardstick::recall_vec(truth = svm_pred[['.pred_class']],\n",
    "                                   estimate = test_outcome),\n",
    "  'Specificity' = yardstick::specificity_vec(truth = svm_pred[['.pred_class']],\n",
    "                                            estimate = test_outcome)\n",
    ") |>\n",
    "  dplyr::bind_cols() |>\n",
    "  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>\n",
    "  dplyr::mutate(Value = round(Value*100, 1))\n",
    "\n",
    "readr::write_csv(x = svm_metrics_table, file = here::here('data', 'svm-metrics.csv'))"
   ],
   "id": "dd315d13-bdfa-4017-b04a-05aa6727d992"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Artificial Neural Network\n",
    "\n",
    "### 4.3.1 Model Description\n",
    "\n",
    "Artificial Neural Networks (ANNs) are a class of machine learning algorithms inspired by the structure and function of the human brain. They consist of interconnected layers of nodes, or neurons, which process input data to perform tasks such as classification, regression, and pattern recognition. ANNs are particularly effective for complex tasks like image and speech recognition, natural language processing, financial forecasting, and medical diagnosis.\n",
    "\n",
    "An ANN is composed of multiple layers, including an input layer, one or more hidden layers, and an output layer. The input layer receives the raw data, the hidden layers process the data through various transformations, and the output layer produces the final prediction or classification. Each connection between neurons has an associated weight, and each neuron has a bias term. These parameters are adjusted during the training process to minimize the error in predictions.\n",
    "\n",
    "The training process of an ANN involves forward propagation, where input data is passed through the network layer by layer. Each neuron applies an activation function to compute its output, introducing non-linearity to help the network learn complex patterns. The loss, or error, between the network’s output and the true target values is calculated using a loss function. Through backpropagation, the loss is propagated backward through the network, and the weights and biases are adjusted using an optimization algorithm like gradient descent.\n",
    "\n",
    "ANNs offer significant advantages, including flexibility in modeling complex relationships and the ability to scale for large datasets and intricate tasks. Their ability to learn and generalize from data makes them powerful tools in various applications, driving advancements in fields ranging from technology and finance to healthcare and beyond.\n",
    "\n",
    "### 4.3.2 Model Workflow"
   ],
   "id": "4df99761-4d9a-47f9-9d55-137360544066"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "✔ broom        1.0.6     ✔ tibble       3.2.1\n",
      "✔ dials        1.2.1     ✔ tidyr        1.3.1\n",
      "✔ ggplot2      3.5.1     ✔ tune         1.2.1\n",
      "✔ infer        1.0.7     ✔ workflows    1.1.4\n",
      "✔ modeldata    1.3.0     ✔ workflowsets 1.1.0\n",
      "✔ purrr        1.0.2     ✔ yardstick    1.3.1\n",
      "✔ rsample      1.2.1     "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "✖ ggplot2::alpha() masks scales::alpha(), kernlab::alpha()\n",
      "✖ purrr::cross()   masks kernlab::cross()\n",
      "✖ purrr::discard() masks scales::discard()\n",
      "✖ dplyr::filter()  masks stats::filter()\n",
      "✖ dplyr::lag()     masks stats::lag()\n",
      "✖ recipes::step()  masks stats::step()\n",
      "• Use suppressPackageStartupMessages() to eliminate package startup messages"
     ]
    }
   ],
   "source": [
    "library(tidymodels)"
   ],
   "id": "aa2d8353-c5ad-4b62-b99f-38cca649e98c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by specifying the ANN model and creating the model workflow. Specifically, we will define a multilayer perceptron model (i.e., a single-layer, feed-forward neural network). The key parameters we will set include the number of epochs (or training iterations), the number of hidden units, the penalty (or weight decay), and the learning rate."
   ],
   "id": "c0e428df-fc54-41ab-acd7-d0481c6c47c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification.\n",
    "ann_model_spec <-\n",
    "  parsnip::mlp(\n",
    "    epochs = tune::tune(),\n",
    "    hidden_units = tune::tune(),\n",
    "    penalty = tune::tune(),\n",
    "    learn_rate = 0.1\n",
    "  ) |>\n",
    "  parsnip::set_engine('nnet') |>\n",
    "  parsnip::set_mode('classification')\n",
    "\n",
    "# Create model workflow.\n",
    "ann_workflow <- workflows::workflow() |>\n",
    "  workflows::add_model(ann_model_spec) |>\n",
    "  workflows::add_recipe(data_rec)"
   ],
   "id": "1cc33be6-fc29-49d4-928d-d64ed771c8cd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Model Tuning and Fitting\n",
    "\n",
    "We will proceed to tune all the parameters except for the learning rate. This is because the `nnet` package does not support tuning the learning rate."
   ],
   "id": "3d8e9c2d-d93e-4554-985b-bbbdd0c835cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15.51 sec elapsed"
     ]
    }
   ],
   "source": [
    "#' Check number of available cores.\n",
    "cores_no <- parallel::detectCores() - 1\n",
    "\n",
    "#' Start timer.\n",
    "tictoc::tic()\n",
    "\n",
    "# Create and register clusters.\n",
    "clusters <- parallel::makeCluster(cores_no)\n",
    "doParallel::registerDoParallel(clusters)\n",
    "\n",
    "# Fine-tune the model params.\n",
    "ann_res <- tune::tune_grid(\n",
    "  object = ann_workflow,\n",
    "  resamples = data_cross_val,\n",
    "  control = tune::control_resamples(save_pred = TRUE)\n",
    ")\n",
    "\n",
    "# Select the best fit based on accuracy.\n",
    "ann_best_fit <- \n",
    "  ann_res |> \n",
    "  tune::select_best(metric = 'accuracy')\n",
    "\n",
    "# Finalize the workflow with the best parameters.\n",
    "ann_final_workflow <- \n",
    "  ann_workflow |>\n",
    "  tune::finalize_workflow(ann_best_fit)\n",
    "\n",
    "# Fit the final model using the best parameters.\n",
    "ann_final_fit <- \n",
    "  ann_final_workflow |> \n",
    "  tune::last_fit(data_split)\n",
    "\n",
    "# Stop clusters.\n",
    "parallel::stopCluster(clusters)\n",
    "\n",
    "# Stop timer.\n",
    "tictoc::toc()"
   ],
   "id": "3a22eee8-6262-42d7-89ca-235c217c061c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Model Performance\n",
    "\n",
    "We then apply our selected model to the test set. The final metrics are given in **?@tbl-ann-performance-html**."
   ],
   "id": "11b1a81c-4e3a-4992-a20f-e5891a3267d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best fit to make predictions on the test data.\n",
    "ann_pred <- \n",
    "  ann_final_fit |> \n",
    "  tune::collect_predictions() |>\n",
    "  dplyr::mutate(truth = factor(.pred_class))"
   ],
   "id": "0fdc91f1-f213-4e90-9c1e-0d54092fa08f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table.\n",
    "ann_metrics_table <- list(\n",
    "  'Accuracy' = yardstick::accuracy_vec(truth = ann_pred[['.pred_class']],\n",
    "                                       estimate = test_outcome),\n",
    "  'Precision' = yardstick::precision_vec(truth = ann_pred[['.pred_class']],\n",
    "                                         estimate = test_outcome),\n",
    "  'Recall' = yardstick::recall_vec(truth = ann_pred[['.pred_class']],\n",
    "                                   estimate = test_outcome),\n",
    "  'Specificity' = yardstick::specificity_vec(truth = ann_pred[['.pred_class']],\n",
    "                                            estimate = test_outcome)\n",
    ") |>\n",
    "  dplyr::bind_cols() |>\n",
    "  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>\n",
    "  dplyr::mutate(Value = round(Value*100, 1))\n",
    "\n",
    "readr::write_csv(x = ann_metrics_table, file = here::here('data', 'ann-metrics.csv'))"
   ],
   "id": "d2934fd3-a9f1-40a5-8e0d-7721e824774e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Logistic Regression\n",
    "\n",
    "### 4.4.1 Model Description\n",
    "\n",
    "Logistic Regression (LR) is a supervised learning algorithm widely used for classification problems. It is particularly effective for binary classification tasks, where the outcome variable can take one of two possible values. The model predicts the probability that a given input belongs to a specific class by applying the logistic (sigmoid) function, which transforms a linear combination of input features into a probability value between $0$ and $1$.\n",
    "\n",
    "For binary classification, the logistic function is defined as $\\sigma(\\hat{Y}_i) = 1/(1 + e^{-\\hat{Y}_i})$ where $\\hat{Y}$ is a linear combination of the input features. The probability of the outcome $i$ being the positive class (represented as 1) is given by:\n",
    "\n",
    "$$\\sigma(\\hat{Y}_i) = \\sigma(\\beta_0 + \\beta_1 X_{1, i} + \\beta_2 X_{2, i} + \\ldots + \\beta_p X_{p, i}),$$\n",
    "\n",
    "where $\\beta_0$ is the intercept, and $\\beta_1, \\beta_2, \\ldots, \\beta_p$ are the coefficients corresponding to the input features $X_1, X_2, \\ldots, X_p$. These coefficients are estimated using the method of maximum likelihood estimation (MLE), which maximizes the likelihood of the observed data.\n",
    "\n",
    "LR can also be extended to handle multi-class classification problems through multinomial logistic regression. In this case, the model uses the softmax function to generalize to multiple classes. The softmax function is an extension of the logistic function for multiple classes and is defined as,\n",
    "\n",
    "$$Pr(Y = j | X = x_0) = \\frac{e^{\\mathbf{\\beta}_j x_0}}{\\sum_{i=1}^{n} e^{\\mathbf{\\beta}_i \\cdot x_0}}$$\n",
    "\n",
    "where $x_0$ is an observation, $n$ is the number of classes, and $\\mathbf{\\beta}_j$ is the coefficient vector for class $j$.\n",
    "\n",
    "The primary advantage of LR is its interpretability. Each coefficient indicates the change in the log-odds of the outcome for a one-unit change in the corresponding predictor variable. This provides clear insights into the influence of each predictor on the probability of the outcome. Despite its simplicity, LR is a powerful tool for both binary and multi-class classification, making it suitable for a wide range of applications where the relationship between the predictors and the log-odds is approximately linear.\n",
    "\n",
    "### 4.4.2 Model Workflow"
   ],
   "id": "29ea0462-139a-43ad-8f06-dea542131ea8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data already computed in the index.qmd file.\n",
    "data_split <- readRDS(here::here('data/data_split.rds'))\n",
    "data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))\n",
    "test_outcome <- readRDS(here::here('data/test_outcome.rds'))\n",
    "data_rec <- readRDS(here::here('data/data_rec.rds'))\n",
    "\n",
    "# Set random seed.\n",
    "set.seed(3145)"
   ],
   "id": "800b7e3a-7861-40d4-b708-1693a48ed962"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train our LR model and find the optimal values for the model parameters. The key parameter we will optimize is the penalty parameter, which refers to the regularization term added to the loss function to prevent overfitting. We will find the optimal penalty value to improve model performance. Additionally, we will set mixture = 1 to apply Lasso regularization, which helps in potentially removing irrelevant predictors and choosing a simpler model."
   ],
   "id": "70f928e7-8106-4866-bbda-f23a035b425b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification.\n",
    "lr_model_spec <-\n",
    "  parsnip::logistic_reg(\n",
    "    penalty = tune(),\n",
    "    mixture = 1) |>\n",
    "  parsnip::set_mode('classification') |>\n",
    "  parsnip::set_engine('glmnet')\n",
    "\n",
    "# Create model workflow.\n",
    "lr_workflow <- workflows::workflow() |>\n",
    "  workflows::add_model(lr_model_spec) |>\n",
    "  workflows::add_recipe(data_rec)"
   ],
   "id": "e8caeba9-af1f-455e-a316-0513831d7d96"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Model Tuning and Fitting"
   ],
   "id": "bd75df26-6424-4b73-8d47-c43454d8abf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10.659 sec elapsed"
     ]
    }
   ],
   "source": [
    "#' Check number of available cores.\n",
    "cores_no <- parallel::detectCores() - 1\n",
    "\n",
    "#' Start timer.\n",
    "tictoc::tic()\n",
    "\n",
    "# Create and register clusters.\n",
    "clusters <- parallel::makeCluster(cores_no)\n",
    "doParallel::registerDoParallel(clusters)\n",
    "\n",
    "# Fine-tune the model params.\n",
    "lr_res <- tune::tune_grid(\n",
    "  object = lr_workflow,\n",
    "  resamples = data_cross_val,\n",
    "  control = tune::control_resamples(save_pred = TRUE)\n",
    ")\n",
    "\n",
    "# Select the best fit based on accuracy.\n",
    "lr_best_fit <- \n",
    "  lr_res |> \n",
    "  tune::select_best(metric = 'accuracy')\n",
    "\n",
    "# Finalize the workflow with the best parameters.\n",
    "lr_final_workflow <- \n",
    "  lr_workflow |>\n",
    "  tune::finalize_workflow(lr_best_fit)\n",
    "\n",
    "# Fit the final model using the best parameters.\n",
    "lr_final_fit <- \n",
    "  lr_final_workflow |> \n",
    "  tune::last_fit(data_split)\n",
    "\n",
    "# Stop clusters.\n",
    "parallel::stopCluster(clusters)\n",
    "\n",
    "# Stop timer.\n",
    "tictoc::toc()"
   ],
   "id": "b7897c0d-8231-4e3d-bbcb-ef71fe65f6f7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Model Performance\n",
    "\n",
    "We then apply our selected model to the test set. The final metrics are given in **?@tbl-lr-performance-html**."
   ],
   "id": "a423995c-d561-4f49-b11d-664ba7c0f2cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best fit to make predictions on the test data.\n",
    "lr_pred <- \n",
    "  lr_final_fit |> \n",
    "  tune::collect_predictions() |>\n",
    "  dplyr::mutate(truth = factor(.pred_class))"
   ],
   "id": "27042be0-1f05-4e20-9a43-802d2f1ad8bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table.\n",
    "lr_metrics_table <- list(\n",
    "  'Accuracy' = yardstick::accuracy_vec(truth = lr_pred[['.pred_class']],\n",
    "                                       estimate = test_outcome),\n",
    "  'Precision' = yardstick::precision_vec(truth = lr_pred[['.pred_class']],\n",
    "                                         estimate = test_outcome),\n",
    "  'Recall' = yardstick::recall_vec(truth = lr_pred[['.pred_class']],\n",
    "                                   estimate = test_outcome),\n",
    "  'Specificity' = yardstick::specificity_vec(truth = lr_pred[['.pred_class']],\n",
    "                                            estimate = test_outcome)\n",
    ") |>\n",
    "  dplyr::bind_cols() |>\n",
    "  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>\n",
    "  dplyr::mutate(Value = round(Value*100, 1))\n",
    "\n",
    "readr::write_csv(x = lr_metrics_table, file = here::here('data', 'lr-metrics.csv'))"
   ],
   "id": "06b001bf-74a6-489b-bffb-ce77329b61d7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Extreme Gradient Boosting\n",
    "\n",
    "### 4.5.1 Model Description\n",
    "\n",
    "Extreme Gradient Boosting (XGBoost) is an advanced implementation of gradient boosting designed to enhance performance and speed. It builds upon the principles of gradient boosting to provide a highly efficient, flexible, and portable library that supports both regression and classification tasks. XGBoost has become one of the most popular machine learning algorithms due to its high performance and scalability.\n",
    "\n",
    "XGBoost operates by sequentially adding decision trees to an ensemble. Each tree is built to correct the errors of the previous trees in the ensemble. The process begins with an initial model, typically a simple model such as the mean of the target variable. At each subsequent step, a new decision tree is added to the model to predict the residuals (errors) of the previous trees. Each tree is built by optimizing an objective function that combines a loss function and a regularization term. The regularization term helps prevent overfitting by penalizing the complexity of the model. After each tree is added, the residuals are updated. The new tree aims to minimize these residuals, improving the overall model’s performance.\n",
    "\n",
    "The node splitting in each tree is guided by an objective function, which typically involves minimizing a loss function (such as mean squared error for regression or log loss for classification) while including a regularization term. The final prediction is the sum of the predictions from all the trees in the ensemble, effectively reducing variance. This process is depicted in the attached flowchart, showing how each tree contributes to the final model.\n",
    "\n",
    "XGBoost has several key advantages. It incorporates both L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting and manage model complexity. The algorithm supports parallel processing, significantly speeding up the training process. XGBoost can handle missing values internally, making it robust to incomplete datasets. Additionally, users can define custom objective functions and evaluation metrics, allowing for flexibility in optimization.\n",
    "\n",
    "### 4.5.2 Model Workflow"
   ],
   "id": "de5c7070-2bcf-4968-bc47-91c2b61ee56e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidymodels)\n",
    "\n",
    "# Load the data already computed in the index.qmd file.\n",
    "data_split <- readRDS(here::here('data/data_split.rds'))\n",
    "data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))\n",
    "test_outcome <- readRDS(here::here('data/test_outcome.rds'))\n",
    "data_rec <- readRDS(here::here('data/data_rec.rds'))\n",
    "\n",
    "# Set random seed.\n",
    "set.seed(3145)"
   ],
   "id": "8190f437-b85f-4643-8820-6de95fbf685b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To effectively train our XGBoost model and find the optimal hyperparameters, we will set up a workflow that includes model specification and data preprocessing. The hyperparameters to be tuned include:\n",
    "\n",
    "-   `tree_depth`: Controls the maximum depth of each tree, impacting the model’s complexity.\n",
    "-   `min_n`: Specifies the minimum number of observations that must exist in a node for a split to be attempted, preventing overly specific branches and encouraging generalization.\n",
    "-   `loss_reduction`: Sets the minimum reduction in the loss function required to make a further partition on a leaf node, helping to control overfitting by making the algorithm more conservative.\n",
    "-   `sample_size`: Determines the fraction of the training data used for fitting each individual tree, introducing randomness and preventing overfitting.\n",
    "-   `mtry`: Sets the number of features considered when looking for the best split, adding variability to enhance generalization.\n",
    "-   `learn_rate`: Also known as the shrinkage parameter, controls the rate at which the model learns. Smaller learning rates can lead to better performance by allowing the model to learn more slowly and avoid overfitting."
   ],
   "id": "e696fc08-a439-4855-96db-00a375ed020e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification.\n",
    "xgboost_model_spec <- \n",
    "  boost_tree(\n",
    "    trees = 1000,\n",
    "    tree_depth = tune(), \n",
    "    min_n = tune(),\n",
    "    loss_reduction = tune(),\n",
    "    sample_size = tune(), \n",
    "    mtry = tune(),\n",
    "    learn_rate = tune()\n",
    "  ) |>\n",
    "  set_engine('xgboost') |>\n",
    "  set_mode('classification')\n",
    "\n",
    "# Create model workflow.\n",
    "xgboost_workflow <- workflows::workflow() |>\n",
    "  workflows::add_model(xgboost_model_spec) |>\n",
    "  workflows::add_recipe(data_rec)"
   ],
   "id": "c85a08f1-dafe-4042-89a5-eb9f69ce45a4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Model Tuning and Fitting"
   ],
   "id": "bd72e20b-1d76-4ff7-b744-f2a569a39e62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "i Creating pre-processing data to finalize unknown parameter: mtry"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21.014 sec elapsed"
     ]
    }
   ],
   "source": [
    "#' Check number of available cores.\n",
    "cores_no <- parallel::detectCores() - 1\n",
    "\n",
    "#' Start timer.\n",
    "tictoc::tic()\n",
    "\n",
    "# Create and register clusters.\n",
    "clusters <- parallel::makeCluster(cores_no)\n",
    "doParallel::registerDoParallel(clusters)\n",
    "\n",
    "# Fine-tune the model params.\n",
    "xgboost_res <- tune::tune_grid(\n",
    "  object = xgboost_workflow,\n",
    "  resamples = data_cross_val,\n",
    "  control = tune::control_resamples(save_pred = TRUE)\n",
    ")"
   ],
   "id": "dbd065c1-63ef-4148-b912-627873858ed4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 Model Performance\n",
    "\n",
    "We then apply our selected model to the test set. The final metrics are given in **?@tbl-xgboost-performance-html**."
   ],
   "id": "ef3cff32-2222-4ff4-b8cd-703609bf4a4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best fit to make predictions on the test data.\n",
    "xgboost_pred <- \n",
    "  xgboost_final_fit |> \n",
    "  tune::collect_predictions() |>\n",
    "  dplyr::mutate(truth = factor(.pred_class))"
   ],
   "id": "7a999f84-4efb-4a1d-8f83-e5d3b359e008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table.\n",
    "xgboost_metrics_table <- list(\n",
    "  'Accuracy' = yardstick::accuracy_vec(truth = xgboost_pred[['.pred_class']],\n",
    "                                       estimate = test_outcome),\n",
    "  'Precision' = yardstick::precision_vec(truth = xgboost_pred[['.pred_class']],\n",
    "                                         estimate = test_outcome),\n",
    "  'Recall' = yardstick::recall_vec(truth = xgboost_pred[['.pred_class']],\n",
    "                                   estimate = test_outcome),\n",
    "  'Specificity' = yardstick::specificity_vec(truth = xgboost_pred[['.pred_class']],\n",
    "                                            estimate = test_outcome)\n",
    ") |>\n",
    "  dplyr::bind_cols() |>\n",
    "  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>\n",
    "  dplyr::mutate(Value = round(Value*100, 1))\n",
    "\n",
    "readr::write_csv(x = xgboost_metrics_table, file = here::here('data', 'xgboost-metrics.csv'))"
   ],
   "id": "a98d7728-9dd3-4081-8f89-80055d109109"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Random Forest\n",
    "\n",
    "### 4.6.1 Model Description\n",
    "\n",
    "Random forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. This method is particularly effective for classification problems, such as the one we are dealing with in the Thyroid dataset where the target variable is categorical. Each decision tree in a random forest splits the predictor space into distinct regions using recursive binary splits. For instance, a tree might first split based on whether $\\text{Age}<35$ and then further split based on whether $\\text{Gender}=\\text{Female}$ to predict cancer recurrence. These splits are chosen to minimize a specific error criterion, such as the Gini index or entropy \\[[13](#ref-james2021)\\].\n",
    "\n",
    "A significant limitation of individual decision trees is their high variance; small changes in the training data can lead to very different tree structures. Random forest addresses this by using bagging, where multiple trees are trained on different bootstrap samples of the data. The final prediction is made by aggregating the predictions of all the trees, typically through majority voting in classification problems. This process reduces variance because the average of many uncorrelated trees’ predictions is less variable than the prediction of a single tree.\n",
    "\n",
    "Random forest further reduces correlation between trees by selecting a random subset of predictors to consider for each split, rather than considering all predictors. Typically, for classification problems, this subset size is approximately $\\sqrt{p}$, where $p$ is the total number of predictors. This random selection of features ensures that the trees are less similar to each other, which reduces the correlation between their predictions and leads to a greater reduction in variance. By combining bagging with feature randomness, random forests create robust models that are less prone to overfitting and provide better generalization to new data.\n",
    "\n",
    "### 4.6.2 Model Workflow"
   ],
   "id": "b4523323-78d6-4c44-9f8b-792d756e8ff2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidymodels)\n",
    "\n",
    "# Load the data already computed in the index.qmd file.\n",
    "data_split <- readRDS(here::here('data/data_split.rds'))\n",
    "data_cross_val <- readRDS(here::here('data/data_cross_val.rds'))\n",
    "test_outcome <- readRDS(here::here('data/test_outcome.rds'))\n",
    "data_rec <- readRDS(here::here('data/data_rec.rds'))\n",
    "\n",
    "# Set random seed.\n",
    "set.seed(3145)"
   ],
   "id": "e2f32f94-ceeb-42fe-a2ca-8f740c6bef2a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will set up a workflow to train our Random Forest model. The goal is to optimize the following hyperparameters to achieve the best performance on our classification task:\n",
    "\n",
    "-   `trees`: This parameter specifies the total number of trees to be grown in the forest. Tuning the number of trees can help ensure that the model is robust and neither overfitting nor underfitting the data.\n",
    "-   `min_n`: This parameter sets the minimum number of observations required in a terminal node. Tuning `min_n` helps control the size of the trees, affecting the model’s ability to generalize to new data."
   ],
   "id": "445e93c4-c96a-4e00-a824-c53d940a1e94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model specification.\n",
    "rf_model_spec <- \n",
    "  parsnip::rand_forest(\n",
    "    trees = 500,\n",
    "    min_n = tune::tune()\n",
    "  ) |>\n",
    "  parsnip::set_engine('ranger') |>\n",
    "  parsnip::set_mode('classification')\n",
    "\n",
    "# Create model workflow.\n",
    "rf_workflow <- workflows::workflow() |>\n",
    "  workflows::add_model(rf_model_spec) |>\n",
    "  workflows::add_recipe(data_rec)"
   ],
   "id": "6630bc40-50b1-423b-8009-5930b1df88dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.3 Model Tuning and Fitting"
   ],
   "id": "e15b63bd-e218-4eaa-99ec-497dd0e98946"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14.251 sec elapsed"
     ]
    }
   ],
   "source": [
    "#' Check number of available cores.\n",
    "cores_no <- parallel::detectCores() - 1\n",
    "\n",
    "#' Start timer.\n",
    "tictoc::tic()\n",
    "\n",
    "# Create and register clusters.\n",
    "clusters <- parallel::makeCluster(cores_no)\n",
    "doParallel::registerDoParallel(clusters)\n",
    "\n",
    "# Fine-tune the model params.\n",
    "rf_res <- tune::tune_grid(\n",
    "  object = rf_workflow,\n",
    "  resamples = data_cross_val,\n",
    "  control = tune::control_resamples(save_pred = TRUE)\n",
    ")\n",
    "\n",
    "# Select the best fit based on accuracy.\n",
    "rf_best_fit <- \n",
    "  rf_res |> \n",
    "  tune::select_best(metric = 'accuracy')\n",
    "\n",
    "# Finalize the workflow with the best parameters.\n",
    "rf_final_workflow <- \n",
    "  rf_workflow |>\n",
    "  tune::finalize_workflow(rf_best_fit)\n",
    "\n",
    "# Fit the final model using the best parameters.\n",
    "rf_final_fit <- \n",
    "  rf_final_workflow |> \n",
    "  tune::last_fit(data_split)\n",
    "\n",
    "# Stop clusters.\n",
    "parallel::stopCluster(clusters)\n",
    "\n",
    "# Stop timer.\n",
    "tictoc::toc()"
   ],
   "id": "f958cb9c-6349-4cd2-a7da-22d661687869"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.4 Model Performance\n",
    "\n",
    "We then apply our selected model to the test set. The final metrics are given in **?@tbl-rf-performance-html**."
   ],
   "id": "d4e0aa13-2bce-4887-868b-78b35ae87365"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best fit to make predictions on the test data.\n",
    "rf_pred <- \n",
    "  rf_final_fit |> \n",
    "  tune::collect_predictions() |>\n",
    "  dplyr::mutate(truth = factor(.pred_class))"
   ],
   "id": "1b321419-74f3-4feb-8a42-fdd19658157a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table.\n",
    "rf_metrics_table <- list(\n",
    "  'Accuracy' = yardstick::accuracy_vec(truth = rf_pred[['.pred_class']],\n",
    "                                       estimate = test_outcome),\n",
    "  'Precision' = yardstick::precision_vec(truth = rf_pred[['.pred_class']],\n",
    "                                         estimate = test_outcome),\n",
    "  'Recall' = yardstick::recall_vec(truth = rf_pred[['.pred_class']],\n",
    "                                   estimate = test_outcome),\n",
    "  'Specificity' = yardstick::specificity_vec(truth = rf_pred[['.pred_class']],\n",
    "                                            estimate = test_outcome)\n",
    ") |>\n",
    "  dplyr::bind_cols() |>\n",
    "  tidyr::pivot_longer(cols = dplyr::everything(), names_to = 'Metric', values_to = 'Value') |>\n",
    "  dplyr::mutate(Value = round(Value*100, 1))\n",
    "\n",
    "readr::write_csv(x = rf_metrics_table, file = here::here('data', 'rf-metrics.csv'))"
   ],
   "id": "35afcbbc-7768-4393-8fd0-08d60b26a837"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Comparison\n",
    "\n",
    "To assess model performance, we will compare the accuracy, precision, recall, and specificity metrics of our models. These metrics are defined as follows.\n",
    "\n",
    "-   Accuracy: proportion of correct predictions for the test data or $\\frac{TN + TP}{TN + TP + FN + FP}$.\n",
    "\n",
    "-   Precision: proportion of correctly classified positive observations among all observations that are classified as positive by the model or $\\frac{TP}{TP + FP}$.\n",
    "\n",
    "-   Recall: proportion of correctly classified positive observations among all actual positive observations or $\\frac{TP}{TP + FN}$.\n",
    "\n",
    "-   Specificity: proportion of correctly classified negative observations among all actual negative observations or $\\frac{TN}{TN + FP}$."
   ],
   "id": "f66e32db-ef8a-413c-9bff-53127e126a64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Rows: 4 Columns: 2\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (1): Metric\n",
      "dbl (1): Value\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "Rows: 4 Columns: 2\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (1): Metric\n",
      "dbl (1): Value\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "Rows: 4 Columns: 2\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (1): Metric\n",
      "dbl (1): Value\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "Rows: 4 Columns: 2\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (1): Metric\n",
      "dbl (1): Value\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "Rows: 4 Columns: 2\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (1): Metric\n",
      "dbl (1): Value\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "Rows: 4 Columns: 2\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (1): Metric\n",
      "dbl (1): Value\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
     ]
    }
   ],
   "source": [
    "models_performance <-\n",
    "  dplyr::bind_rows(\n",
    "    readr::read_csv('data/knn-metrics.csv') |>\n",
    "      dplyr::mutate('Model' = 'KNN', .before = 1),\n",
    "    readr::read_csv('data/svm-metrics.csv') |>\n",
    "      dplyr::mutate('Model' = 'SVM', .before = 1),\n",
    "    readr::read_csv('data/ann-metrics.csv') |>\n",
    "      dplyr::mutate('Model' = 'ANN', .before = 1),\n",
    "    readr::read_csv('data/lr-metrics.csv') |>\n",
    "      dplyr::mutate('Model' = 'Logistic Regression', .before = 1),\n",
    "    readr::read_csv('data/xgboost-metrics.csv') |>\n",
    "      dplyr::mutate('Model' = 'XGBoost', .before = 1),\n",
    "    readr::read_csv('data/rf-metrics.csv') |>\n",
    "      dplyr::mutate('Model' = 'Random Forest', .before = 1)\n",
    "  ) |>\n",
    "  dplyr::filter(Value == max(Value), .by = Metric) |>\n",
    "  dplyr::reframe(Model = paste0(Model, collapse = ', '), .by = c(Metric, Value)) |>\n",
    "  dplyr::select(Metric, Model, Value) |>\n",
    "  dplyr::arrange(Metric) |>\n",
    "  dplyr::mutate(Value = paste0(round(Value), '%'))"
   ],
   "id": "6f89930f-8ea1-4850-8583-700a11dc20dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated the performance of our models on the Thyroid dataset using the metrics shown in **?@tbl-overall-performance-html**.\n",
    "\n",
    "The Random Forest model emerged as the top performer in terms of overall accuracy and specificity, achieving a $94\\%$ accuracy rate and a $94\\%$ specificity rate, indicating its robustness in correctly identifying negative cases. The precision metric, though the lowest among the metrics, was $85\\%$ across the Artificial Neural Network, Logistic Regression, and Random Forest models, reflecting their ability to correctly classify positive cases. The Support Vector Machine model stood out in terms of recall, achieving a perfect score of $100\\%$, suggesting its effectiveness in capturing all positive cases. These results highlight the Random Forest model as the most balanced and reliable for this classification task, combining high accuracy and specificity with competitive precision, while the SVM model excels in recall, making it particularly suitable for ensuring that all positive cases are identified."
   ],
   "id": "e83b3bc9-8306-4c1f-b83c-05ef6bd44240"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- ```{r model-metrics} -->"
   ],
   "id": "3a816b7b-8399-4fb2-80a3-6a06d5948e63"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- #| label: tbl-overall-performance -->"
   ],
   "id": "2322734d-b003-4e70-8ea8-f66e6fbf9d16"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- #| tbl-cap: \"Performance Metrics for KNN, SVM, and ANN in Terms of Accuracy, Precision, Recall, and Specificity\" -->"
   ],
   "id": "c67e7960-4842-4ea2-9165-7204fb66bae7"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- #| tbl-alt: \"Performance Metrics for KNN, SVM, and ANN in Terms of Accuracy, Precision, Recall, and Specificity\" -->"
   ],
   "id": "a46d3893-6558-45cc-beb4-93112d9386fb"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- # Compile metrics for each model. -->"
   ],
   "id": "bdbd1c2c-c60b-4b7f-ab71-c6bfdd6df219"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- models_performance <- -->"
   ],
   "id": "e6ba8f7d-87b9-4f84-aea8-3e377ecaa49a"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--   dplyr::bind_rows( -->"
   ],
   "id": "0ae30291-7b5c-4083-9c16-cb179d2a7269"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     readr::read_csv('data/knn-metrics.csv') |> -->"
   ],
   "id": "a1b468f5-c65f-4d79-9607-d91eb034cf20"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--       dplyr::mutate('Model' = 'KNN', .before = 1), -->"
   ],
   "id": "f66d4c43-0d43-4a18-a83d-115738935c72"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     readr::read_csv('data/svm-metrics.csv') |> -->"
   ],
   "id": "83020598-37fa-4de2-a468-cdf5d51a092f"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--       dplyr::mutate('Model' = 'SVM', .before = 1), -->"
   ],
   "id": "1a662075-49e5-4dd8-9022-c29ba0f00790"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     readr::read_csv('data/ann-metrics.csv') |> -->"
   ],
   "id": "4331e8a2-8518-4706-86ba-461e4d43019b"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--       dplyr::mutate('Model' = 'ANN', .before = 1) -->"
   ],
   "id": "b67acef8-19ff-4d29-b014-83f330580f76"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--   ) -->"
   ],
   "id": "634842f0-fe0b-4b0e-8bbe-da9ef585b205"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- # Prepare table's theme. -->"
   ],
   "id": "9233f3c8-db34-4934-a6e1-71ac3f7cb107"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- theme <- reactable::reactableTheme( -->"
   ],
   "id": "789c1a3c-dfd2-42b5-a38d-def50d29fd42"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--   borderColor = \"#dfe2e5\", -->"
   ],
   "id": "9b4789fb-215e-4683-9a86-d01b2e93e8c1"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--   stripedColor = \"#f6f8fa\", -->"
   ],
   "id": "993e15df-f18f-421c-af5e-33ca1eb4d8cb"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--   highlightColor = \"#f0f5f9\", -->"
   ],
   "id": "54ef3036-0d09-4cff-b52d-0b3a99008706"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--   cellPadding = \"8px 12px\" -->"
   ],
   "id": "4d37a6a0-a949-4f3c-95f0-a0b2ce7f262f"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- ) -->"
   ],
   "id": "096a79cb-4716-4b9e-8cdd-5a49a1d58dcc"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- # Show models' performance. -->"
   ],
   "id": "3924c8c9-6b41-41ee-b774-17da14bd6767"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- models_performance |> -->"
   ],
   "id": "6025619d-17c3-42a8-931a-0ff6afc00889"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--   dplyr::mutate(Value = paste0(Value, '%')) |> -->"
   ],
   "id": "608b813e-0a09-4506-b918-e4927034cb44"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--   reactable::reactable( -->"
   ],
   "id": "6a90d99d-87c7-4f25-8d68-6e8dd324e36c"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     defaultExpanded = TRUE, -->"
   ],
   "id": "fae3555e-308b-4d2c-a2d1-23976c929235"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     groupBy = 'Model', -->"
   ],
   "id": "73b2d746-ac8a-4d52-8268-293433dc3045"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     searchable = FALSE, -->"
   ],
   "id": "a6c0cc7e-a24b-4d8e-b297-a1211f07ec3e"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     resizable = TRUE, -->"
   ],
   "id": "460e2232-5154-42ea-aae9-81a7f704d889"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     onClick = \"expand\", -->"
   ],
   "id": "bc32e4fb-d711-4d94-b397-243075898116"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     bordered = TRUE, -->"
   ],
   "id": "986e74e3-fcb4-4950-90df-2769ce659612"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     highlight = TRUE, -->"
   ],
   "id": "ebe022b6-dca7-4963-b5c7-555f51c19d98"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     compact = TRUE, -->"
   ],
   "id": "7372b834-4982-43de-8ef1-092fd4068cc9"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     height = \"auto\", -->"
   ],
   "id": "0dd2a2a9-bfe1-40f2-8a50-1651cac8c753"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--     theme = theme -->"
   ],
   "id": "fd524c8c-6048-4b26-b81a-4abbea2c104f"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--   ) -->"
   ],
   "id": "ef1d501a-7cc4-49a6-87c7-5db353ba7bce"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- ``` -->"
   ],
   "id": "4969b16e-3a89-4f55-a4e9-f8f930e01983"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "7cb9fe34-e8a3-4114-a9a0-e0bae81816e5"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!---------------------------- ## CONCLUSION ---------------------------------->"
   ],
   "id": "45b4dd9f-953e-440c-8bbc-7f2827dd287c"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "77282be7-e68f-4551-8387-4b0d3c43a08b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "This study aimed to evaluate and compare the effectiveness of various machine learning models in predicting the recurrence of Differentiated Thyroid Cancer (DTC). By examining key metrics such as accuracy, precision, recall, and specificity, we assessed the performance of models including Artificial Neural Network (ANN), Logistic Regression (LR), K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Random Forest (RF), and Extreme Gradient Boosting (XGBoost).\n",
    "\n",
    "Our findings indicate that the Random Forest model is the most robust and balanced classifier for this task. It achieved the highest accuracy and specificity rates, both at $94\\%$, demonstrating its reliability in correctly identifying both positive and negative cases. This suggests that Random Forest is highly effective in distinguishing between patients who will and will not experience a recurrence of thyroid cancer.\n",
    "\n",
    "In terms of precision, the ANN, Logistic Regression, and Random Forest models all achieved an $85\\%$ precision rate. This shows that these models are equally competent in accurately predicting positive cases among those identified as positive, minimizing false positives.\n",
    "\n",
    "The SVM model excelled in recall, achieving a perfect score of $100\\%$. This indicates that SVM is exceptionally effective at capturing all actual positive cases, making it a critical tool when the primary goal is to ensure that no positive cases are missed. This is particularly important in medical diagnostics where missing a positive case can have serious implications.\n",
    "\n",
    "Overall, while the Random Forest model provides the best balance of performance across all metrics, the SVM model’s outstanding recall rate highlights its utility in scenarios where it is crucial to identify all positive cases. These results underscore the importance of selecting the appropriate model based on the specific needs of the task. For balanced performance and overall robustness, Random Forest is recommended. However, for applications where recall is paramount, SVM is the superior choice.\n",
    "\n",
    "Future work can explore the integration of these models in a hybrid approach, leveraging the strengths of each to further improve prediction accuracy and reliability. Additionally, investigating the impact of different feature engineering techniques and more sophisticated hyperparameter tuning methods may yield further enhancements in model performance.\n",
    "\n",
    "These findings contribute valuable insights into the application of machine learning in medical diagnostics, particularly for predicting the recurrence of DTC, and pave the way for more personalized and accurate treatment strategies."
   ],
   "id": "6898eecb-b816-4df9-8cd0-69fa908ac6b5"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "0c5fa8b1-1a5c-4ad3-b3a4-661f5d110d61"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!------------------------- ## INFORMED CONSENT ------------------------------->"
   ],
   "id": "e54b3a8a-e73a-4d5e-ab55-69b9862b4c45"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "ec652f95-469a-4491-831b-c5dc0d47a56d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Informed Consent\n",
    "\n",
    "We used anonymous data for modeling and no consent was required for conducting this study."
   ],
   "id": "02d242bf-38af-4b71-835b-9a750c5ca01d"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "3a19f309-7fbe-4119-99a4-15bed49a15f7"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------- ## REVIEW BOARD ------------------------------->"
   ],
   "id": "6562add1-2500-40ee-842d-e859a14822a4"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "cf28b99f-44c1-42fe-84e9-396ff319f128"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Institutional Review Board Statement\n",
    "\n",
    "Not applicable"
   ],
   "id": "430ec754-395e-40ed-8a4e-2617274757de"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "3fb7507d-c5a4-49d8-bb3a-262dacbd1825"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------- ## FUNDING ------------------------------------>"
   ],
   "id": "8fb20097-f981-4ac4-921d-30c83088cd75"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "9552aa1f-fde0-4910-8fb3-3ea7d2f5f855"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Funding\n",
    "\n",
    "There was no funding for conducting this study."
   ],
   "id": "1b7f0611-f4b7-4355-9d49-c7b41508be0e"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "c0cb2183-ca3c-4ac2-9d98-24545d826af2"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!------------------------ ## DATA AVAILABILITY ------------------------------->"
   ],
   "id": "acf36500-448d-4d35-9c47-4298cb353c1d"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "208f1baf-25ed-4529-bfea-a1c249758838"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Data Availability Statement\n",
    "\n",
    "The data is available at the UC Irvine Machine Learning Repository \\[[12](#ref-ucidata)\\]. For more information, please refer to \\[[4](#ref-borzooei2023)\\]."
   ],
   "id": "bd2cc87f-ae37-49dd-98fc-38f94ab54e56"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "83e96a94-3f6a-4af8-971e-57c49b6192ef"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------- ## ACKNOWLEDGMENTS ---------------------------------->"
   ],
   "id": "b322e149-73c1-406e-bccb-9cdfc7068cea"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "b1bc55b3-12e8-4291-986f-89778aa1ceab"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Acknowledgments\n",
    "\n",
    "We thank our PRIMES mentor, Dr. Marly Gotti, for her guidance throughout the reading and research periods. We are also grateful to the PRIMES organizers for making this program possible."
   ],
   "id": "580c4dd5-05be-4f5a-88cf-b421a22d81b0"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "e22d704b-530a-43f4-ad05-4e0677993ffa"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--------------------------- ## CONFLICTS ------------------------------------>"
   ],
   "id": "20de95d5-19b5-4016-a6b3-ada5bd711e47"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "16fa6ada-d869-4418-af78-bd33bc2592e8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Conflicts of Interest\n",
    "\n",
    "None"
   ],
   "id": "3534ae86-2545-44e3-9f94-8da04e69c017"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "74328101-db13-431e-beeb-07dffb524350"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!------------------------- ## ABBREVIATIONS ---------------------------------->"
   ],
   "id": "89879b0d-8830-49ff-a87d-ffdfca9284d6"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "d755e572-bb6f-44c2-bc54-b2db429fe24a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Abbreviations\n",
    "\n",
    "The following abbreviations are used in this manuscript:\n",
    "\n",
    "-   KNN: K-Nearest Neighbors\n",
    "-   SVM: Support Vector Machine\n",
    "-   RBF: Radial Basis Function\n",
    "-   ANN: Artificial Neural Network\n",
    "-   NNs: Neural Network(s)\n",
    "-   RF: Random Forest\n",
    "-   LR: Logistic Regression\n",
    "-   XGBoost: Extreme Gradient Boosting\n",
    "-   DTC: Differentiated Thyroid Cancer\n",
    "-   ML: Machine Learning"
   ],
   "id": "835b20a5-9392-4489-8cb8-16dc8f2b91d2"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "17d6f5d9-047d-4c94-8dbb-371f30c32e7a"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-------------------------- ## REFERENCES ------------------------------------>"
   ],
   "id": "843bc936-5626-4528-856a-fd6f2f65521c"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!----------------------------------------------------------------------------->"
   ],
   "id": "f6fd64d9-62e1-49d3-830b-5a5d48321929"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[1\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Pellegriti</span>, G., <span class=\"smallcaps\">Frasca</span>, F., <span class=\"smallcaps\">Regalbuto</span>, C., <span class=\"smallcaps\">Squatrito</span>, S. and <span class=\"smallcaps\">Vigneri</span>, R. (2013). [Worldwide increasing incidence of thyroid cancer: Update on epidemiology and risk factors](https://doi.org/10.1155/2013/965212). *Journal of Cancer Epidemiology* **2013** 1–10.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[2\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Bhattacharya</span>, S., <span class=\"smallcaps\">Mahato</span>, R. K., <span class=\"smallcaps\">Singh</span>, S., <span class=\"smallcaps\">Bhatti</span>, G. K., <span class=\"smallcaps\">Mastana</span>, S. S. and <span class=\"smallcaps\">Bhatti</span>, J. S. (2023). [Advances and challenges in thyroid cancer: The interplay of genetic modulators, targeted therapies, and AI-driven approaches](https://doi.org/10.1016/j.lfs.2023.122110). *Life Sciences* **332** 122110.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[3\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">NM</span>, X., <span class=\"smallcaps\">L</span>, W. and <span class=\"smallcaps\">C.</span>, Y. (2022). [Improving the diagnosis of thyroid cancer by machine learning and clinical data](https://doi.org/10.1038/s41598-022-15342-z). *Scientific report* **12** 11143.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[4\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Borzooei</span>, S. and <span class=\"smallcaps\">Tarokhian</span>, A. (2023). [Differentiated thyroid cancer recurrence](https://doi.org/10.24432/C5632J). *UCI Machine Learning Repository*.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[5\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Abiodun</span>, O. I., <span class=\"smallcaps\">Jantan</span>, A., <span class=\"smallcaps\">Omolara</span>, A. E., <span class=\"smallcaps\">Dada</span>, K. V., <span class=\"smallcaps\">Mohamed</span>, N. A. and <span class=\"smallcaps\">Arshad</span>, H. (2018). [State-of-the-art in artificial neural network applications: A survey](https://doi.org/10.1016/j.heliyon.2018.e00938). *Heliyon* **4** e00938.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[6\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Cervantes</span>, J., <span class=\"smallcaps\">Garcia-Lamont</span>, F., <span class=\"smallcaps\">Rodríguez-Mazahua</span>, L. and <span class=\"smallcaps\">Lopez</span>, A. (2020). [A comprehensive survey on support vector machine classification: Applications, challenges and trends](https://doi.org/10.1016/j.neucom.2019.10.118). *Neurocomputing* **408** 189–215.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[7\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Syriopoulos</span>, P. K., <span class=\"smallcaps\">Kotsiantis</span>, S. B. and <span class=\"smallcaps\">Vrahatis</span>, M. N. (2022). [Survey on KNN methods in data science](https://doi.org/10.1007/978-3-031-24866-5_28). In *Learning and intelligent optimization* (D. E. Simos, V. A. Rasskazova, F. Archetti, I. S. Kotsireas and P. M. Pardalos, ed) pp 379–93. Springer International Publishing, Cham.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[8\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Maalouf</span>, M. (2011). [Logistic regression in data analysis: An overview](https://doi.org/10.1504/IJDATS.2011.041335). *International Journal of Data Analysis Techniques and Strategies* **3** 281–99.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[9\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Genuer</span>, R., <span class=\"smallcaps\">Poggi</span>, J.-M. and <span class=\"smallcaps\">Tuleau-Malot</span>, C. (2010). [Variable selection using random forests](https://doi.org/10.1016/j.patrec.2010.03.014). *Pattern Recognition Letters* **31** 2225–36.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[10\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Anju</span> and <span class=\"smallcaps\">Sharma</span>, N. (2018). [Survey of boosting algorithms for big data applications](https://www.ijert.org/survey-of-boosting-algorithms-for-big-data-applications). *International Journal of Engineering Research and Technology (IJERT)* **5**.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[11\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Kourou</span>, K., <span class=\"smallcaps\">Exarchos</span>, T. P., <span class=\"smallcaps\">Exarchos</span>, K. P., <span class=\"smallcaps\">Karamouzis</span>, M. V. and <span class=\"smallcaps\">Fotiadis</span>, D. I. (2015). [Machine learning applications in cancer prognosis and prediction](https://doi.org/10.1016/j.csbj.2014.11.005). *Computational and Structural Biotechnology Journal* **13** 8–17.</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[12\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">Kelly</span>, R., Markelle andLongjohn and <span class=\"smallcaps\">Nottingham</span>, K. [The UCI machine learning repository](https://archive.ics.uci.edu).</span>\n",
    "\n",
    "<span class=\"csl-left-margin\">\\[13\\] </span><span class=\"csl-right-inline\"><span class=\"smallcaps\">G. James</span>, T. H., D. Witten and <span class=\"smallcaps\">Tibshirani</span>, R. (2021). *An introduction to statistical learning*. Springer Texts in Statistics.</span>"
   ],
   "id": "fadc2cd0-06c9-4a32-b557-4152291e741a"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
